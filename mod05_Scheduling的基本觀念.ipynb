{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c73690",
   "metadata": {},
   "source": [
    "# Scheduling, Preemption and Eviction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4823e3f",
   "metadata": {},
   "source": [
    "In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of terminating one or more Pods on Nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043d921",
   "metadata": {},
   "source": [
    "# Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9938ea",
   "metadata": {},
   "source": [
    "### Kubernetes Scheduler"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18e3373e",
   "metadata": {},
   "source": [
    "In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that Kubelet can run them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59b35a",
   "metadata": {},
   "source": [
    "### Scheduling overview"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07fd94f9",
   "metadata": {},
   "source": [
    "A scheduler watches for newly created Pods that have no Node assigned. For every Pod that the scheduler discovers, the scheduler becomes responsible for finding the best Node for that Pod to run on. The scheduler reaches this placement decision taking into account the scheduling principles described below.\n",
    "\n",
    "If you want to understand why Pods are placed onto a particular Node, or if you're planning to implement a custom scheduler yourself, this page will help you learn about scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647bca6e",
   "metadata": {},
   "source": [
    "### kube-scheduler"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f7b4d7b",
   "metadata": {},
   "source": [
    "kube-scheduler is the default scheduler for Kubernetes and runs as part of the control plane. kube-scheduler is designed so that, if you want and need to, you can write your own scheduling component and use that instead.\n",
    "\n",
    "For every newly created pod or other unscheduled pods, kube-scheduler selects an optimal node for them to run on. However, every container in pods has different requirements for resources and every pod also has different requirements. Therefore, existing nodes need to be filtered according to the specific scheduling requirements.\n",
    "\n",
    "In a cluster, Nodes that meet the scheduling requirements for a Pod are called feasible nodes. If none of the nodes are suitable, the pod remains unscheduled until the scheduler is able to place it.\n",
    "\n",
    "The scheduler finds feasible Nodes for a Pod and then runs a set of functions to score the feasible Nodes and picks a Node with the highest score among the feasible ones to run the Pod. The scheduler then notifies the API server about this decision in a process called binding.\n",
    "\n",
    "Factors that need to be taken into account for scheduling decisions include individual and collective resource requirements, hardware / software / policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51477ae4",
   "metadata": {},
   "source": [
    "### Node selection in kube-scheduler "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7c832be",
   "metadata": {},
   "source": [
    "kube-scheduler selects a node for the pod in a 2-step operation:\n",
    "\n",
    "1. Filtering\n",
    "2. Scoring\n",
    "\n",
    "The filtering step finds the set of Nodes where it's feasible to schedule the Pod. For example, the PodFitsResources filter checks whether a candidate Node has enough available resource to meet a Pod's specific resource requests. After this step, the node list contains any suitable Nodes; often, there will be more than one. If the list is empty, that Pod isn't (yet) schedulable.\n",
    "\n",
    "In the scoring step, the scheduler ranks the remaining nodes to choose the most suitable Pod placement. The scheduler assigns a score to each Node that survived filtering, basing this score on the active scoring rules.\n",
    "\n",
    "Finally, kube-scheduler assigns the Pod to the Node with the highest ranking. If there is more than one node with equal scores, kube-scheduler selects one of these at random.\n",
    "\n",
    "There are two supported ways to configure the filtering and scoring behavior of the scheduler:\n",
    "\n",
    "1. Scheduling Policies allow you to configure Predicates for filtering and Priorities for scoring.\n",
    "2. Scheduling Profiles allow you to configure Plugins that implement different scheduling stages, including: QueueSort, \n",
    "   Filter, Score, Bind, Reserve, Permit, and others. You can also configure the kube-scheduler to run different profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db0e84",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl run web --image=nginx\n",
    "kubectl describe pod web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d813ad",
   "metadata": {},
   "source": [
    "<img src='./img/29.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385afd9",
   "metadata": {},
   "source": [
    "# Assigning Pods to Nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dba338bb",
   "metadata": {},
   "source": [
    "You can constrain a Pod so that it is restricted to run on particular node(s), or to prefer to run on particular nodes. There are several ways to do this and the recommended approaches all use label selectors to facilitate the selection. Often, you do not need to set any such constraints; the scheduler will automatically do a reasonable placement (for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources). However, there are some circumstances where you may want to control which node the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it, or to co-locate Pods from two different services that communicate a lot into the same availability zone.\n",
    "\n",
    "You can use any of the following methods to choose where Kubernetes schedules specific Pods:\n",
    "\n",
    "1. nodeSelector field matching against node labels\n",
    "2. Affinity and anti-affinity\n",
    "3. nodeName field\n",
    "4. Pod topology spread constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821a3fd",
   "metadata": {},
   "source": [
    "### Node labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef3bf5c3",
   "metadata": {},
   "source": [
    "Like many other Kubernetes objects, nodes have labels. You can attach labels manually. Kubernetes also populates a standard set of labels on all nodes in a cluster. See Well-Known Labels, Annotations and Taints for a list of common node labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b76dc",
   "metadata": {},
   "source": [
    "### Node isolation/restriction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9770f98e",
   "metadata": {},
   "source": [
    "Adding labels to nodes allows you to target Pods for scheduling on specific nodes or groups of nodes. You can use this functionality to ensure that specific Pods only run on nodes with certain isolation, security, or regulatory properties.\n",
    "\n",
    "If you use labels for node isolation, choose label keys that the kubelet cannot modify. This prevents a compromised node from setting those labels on itself so that the scheduler schedules workloads onto the compromised node.\n",
    "\n",
    "The NodeRestriction admission plugin prevents the kubelet from setting or modifying labels with a node-restriction.kubernetes.io/ prefix.\n",
    "\n",
    "To make use of that label prefix for node isolation:\n",
    "\n",
    "1. Ensure you are using the Node authorizer and have enabled the NodeRestriction admission plugin.\n",
    "2. Add labels with the node-restriction.kubernetes.io/ prefix to your nodes, and use those labels in your node selectors. \n",
    "   For example, example.com.node-restriction.kubernetes.io/fips=true or example.com.node-restriction.kubernetes.io/pci-\n",
    "   dss=true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80809aa4",
   "metadata": {},
   "source": [
    "### nodeSelector"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da447614",
   "metadata": {},
   "source": [
    "nodeSelector is the simplest recommended form of node selection constraint. You can add the nodeSelector field to your Pod specification and specify the node labels you want the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b74816",
   "metadata": {},
   "source": [
    "### 範例: Assign Pods to Nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7b44466",
   "metadata": {},
   "source": [
    "This page shows how to assign a Kubernetes Pod to a particular node in a Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f48f20",
   "metadata": {},
   "source": [
    "##### Before you begin"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a991f20",
   "metadata": {},
   "source": [
    "You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:\n",
    "\n",
    "1. Killercoda\n",
    "2. Play with Kubernetes\n",
    "To check the version, enter kubectl version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e4f60",
   "metadata": {},
   "source": [
    "##### Add a label to a node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cf7d387",
   "metadata": {},
   "source": [
    "List the nodes in your cluster, along with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get nodes --show-labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db9bca64",
   "metadata": {},
   "source": [
    "The output is similar to this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99d32923",
   "metadata": {},
   "source": [
    "NAME      STATUS    ROLES    AGE     VERSION        LABELS\n",
    "worker0   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker0\n",
    "worker1   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker1\n",
    "worker2   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbdff079",
   "metadata": {},
   "source": [
    "Choose one of your nodes, and add a label to it.\n",
    "where <your-node-name> is the name of your chosen node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl label nodes <your-node-name> disktype=ssd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2fc11cf9",
   "metadata": {},
   "source": [
    "Verify that your chosen node has a disktype=ssd label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18423e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get nodes --show-labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd27e38c",
   "metadata": {},
   "source": [
    "The output is similar to this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75d46300",
   "metadata": {},
   "source": [
    "NAME      STATUS    ROLES    AGE     VERSION        LABELS\n",
    "worker0   Ready     <none>   1d      v1.13.0        ...,disktype=ssd,kubernetes.io/hostname=worker0\n",
    "worker1   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker1\n",
    "worker2   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7effcc6f",
   "metadata": {},
   "source": [
    "In the preceding output, you can see that the worker0 node has a disktype=ssd label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904fb00",
   "metadata": {},
   "source": [
    "##### 🔥 Create a pod that gets scheduled to your chosen node 🔥"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f239b49",
   "metadata": {},
   "source": [
    "This pod configuration file describes a pod that has a node selector, disktype: ssd. This means that the pod will get scheduled on a node that has a disktype=ssd label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "  labels:\n",
    "    env: test\n",
    "spec:\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "    imagePullPolicy: IfNotPresent\n",
    "  nodeSelector:\n",
    "    disktype: ssd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c15c908",
   "metadata": {},
   "source": [
    "Use the configuration file to create a pod that will get scheduled on your chosen node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a246dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3231cb8",
   "metadata": {},
   "source": [
    "Verify that the pod is running on your chosen node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270da3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --output=wide"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f19f8fb6",
   "metadata": {},
   "source": [
    "The output is similar to this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee24473c",
   "metadata": {},
   "source": [
    "NAME     READY     STATUS    RESTARTS   AGE    IP           NODE\n",
    "nginx    1/1       Running   0          13s    10.200.0.4   worker0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92f54a28",
   "metadata": {},
   "source": [
    "注意如果在產生 pod 的 yaml 檔加入了關鍵字 nodeSelector，如果沒有任何一個 node 滿足條件的話，pod 會 pending。等到我們把 node 進行 label 滿足條件後就會 running。只要是 pod 已經啟動後，這時候在把 node 的 label 拿掉不會有影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4302cbe3",
   "metadata": {},
   "source": [
    "##### 🔥 Create a pod that gets scheduled to specific node 🔥"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d66a72fc",
   "metadata": {},
   "source": [
    "You can also schedule a pod to one specific node via setting nodeName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2fe62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "spec:\n",
    "  nodeName: foo-node # schedule pod to specific node\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "    imagePullPolicy: IfNotPresent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ef4e4f2",
   "metadata": {},
   "source": [
    "Use the configuration file to create a pod that will get scheduled on foo-node only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935c7bd",
   "metadata": {},
   "source": [
    "# Affinity and anti-affinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e5d3193",
   "metadata": {},
   "source": [
    "nodeSelector is the simplest way to constrain Pods to nodes with specific labels. Affinity and anti-affinity expands the types of constraints you can define. Some of the benefits of affinity and anti-affinity include:\n",
    "\n",
    "1. The affinity/anti-affinity language is more expressive. nodeSelector only selects nodes with all the specified labels. \n",
    "   Affinity/anti-affinity gives you more control over the selection logic.\n",
    "2. You can indicate that a rule is soft or preferred, so that the scheduler still schedules the Pod even if it can't find a \n",
    "   matching node.\n",
    "3. You can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just \n",
    "   node labels, which allows you to define rules for which Pods can be co-located on a node.\n",
    "   \n",
    "The affinity feature consists of two types of affinity:\n",
    "\n",
    "1. Node affinity functions like the nodeSelector field but is more expressive and allows you to specify soft rules.\n",
    "2. Inter-pod affinity/anti-affinity allows you to constrain Pods against labels on other Pods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7774a0",
   "metadata": {},
   "source": [
    "### Node affinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "985d9d62",
   "metadata": {},
   "source": [
    "Node affinity is conceptually similar to nodeSelector, allowing you to constrain which nodes your Pod can be scheduled on based on node labels. There are two types of node affinity:\n",
    "\n",
    "1. requiredDuringSchedulingIgnoredDuringExecution: The scheduler can't schedule the Pod unless the rule is met. This \n",
    "   functions like nodeSelector, but with a more expressive syntax.\n",
    "2. preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets the rule. If a matching \n",
    "   node is not available, the scheduler still schedules the Pod."
   ]
  },
  {
   "cell_type": "raw",
   "id": "72c93270",
   "metadata": {},
   "source": [
    "You can specify node affinities using the .spec.affinity.nodeAffinity field in your Pod spec.\n",
    "For example, consider the following Pod spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: with-node-affinity\n",
    "spec:\n",
    "  affinity:\n",
    "    nodeAffinity:\n",
    "      requiredDuringSchedulingIgnoredDuringExecution:\n",
    "        nodeSelectorTerms:\n",
    "        - matchExpressions:\n",
    "          - key: topology.kubernetes.io/zone\n",
    "            operator: In\n",
    "            values:\n",
    "            - antarctica-east1\n",
    "            - antarctica-west1\n",
    "      preferredDuringSchedulingIgnoredDuringExecution:\n",
    "      - weight: 1\n",
    "        preference:\n",
    "          matchExpressions:\n",
    "          - key: another-node-label-key\n",
    "            operator: In\n",
    "            values:\n",
    "            - another-node-label-value\n",
    "  containers:\n",
    "  - name: with-node-affinity\n",
    "    image: registry.k8s.io/pause:2.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1c250ca",
   "metadata": {},
   "source": [
    "In this example, the following rules apply:\n",
    "\n",
    "1. The node must have a label with the key topology.kubernetes.io/zone and the value of that label must be either \n",
    "   antarctica-east1 or antarctica-west1.\n",
    "2. The node preferably has a label with the key another-node-label-key and the value another-node-label-value.\n",
    "\n",
    "You can use the operator field to specify a logical operator for Kubernetes to use when interpreting the rules. You can use In, NotIn, Exists, DoesNotExist, Gt and Lt.\n",
    "\n",
    "NotIn and DoesNotExist allow you to define node anti-affinity behavior. Alternatively, you can use node taints to repel Pods from specific nodes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "24e991ba",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1. If you specify both nodeSelector and nodeAffinity, both must be satisfied for the Pod to be scheduled onto a node.\n",
    "\n",
    "2. If you specify multiple nodeSelectorTerms associated with nodeAffinity types, then the Pod can be scheduled onto a node \n",
    "   if one of the specified nodeSelectorTerms can be satisfied.\n",
    "\n",
    "3. If you specify multiple matchExpressions associated with a single nodeSelectorTerms, then the Pod can be scheduled onto \n",
    "   a node only if all the matchExpressions are satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323c57f",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eabfe61c",
   "metadata": {},
   "source": [
    "This example shows how to assign a Kubernetes Pod to a particular node using Node Affinity in a Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602cf7f2",
   "metadata": {},
   "source": [
    "##### Add a label to a node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "936494be",
   "metadata": {},
   "source": [
    "List the nodes in your cluster, along with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get nodes --show-labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4a71cb1",
   "metadata": {},
   "source": [
    "The output is similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88874d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME      STATUS    ROLES    AGE     VERSION        LABELS\n",
    "worker0   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker0\n",
    "worker1   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker1\n",
    "worker2   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "121b4120",
   "metadata": {},
   "source": [
    "Choose one of your nodes, and add a label to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl label nodes <your-node-name> disktype=ssd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e77d0694",
   "metadata": {},
   "source": [
    "Verify that your chosen node has a disktype=ssd label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get nodes --show-labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "998ad80b",
   "metadata": {},
   "source": [
    "The output is similar to this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a760c4bc",
   "metadata": {},
   "source": [
    "NAME      STATUS    ROLES    AGE     VERSION        LABELS\n",
    "worker0   Ready     <none>   1d      v1.13.0        ...,disktype=ssd,kubernetes.io/hostname=worker0\n",
    "worker1   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker1\n",
    "worker2   Ready     <none>   1d      v1.13.0        ...,kubernetes.io/hostname=worker2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3143e195",
   "metadata": {},
   "source": [
    "In the preceding output, you can see that the worker0 node has a disktype=ssd label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d1303",
   "metadata": {},
   "source": [
    "##### Schedule a Pod using required node affinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffce047d",
   "metadata": {},
   "source": [
    "This manifest describes a Pod that has a requiredDuringSchedulingIgnoredDuringExecution node affinity,disktype: ssd. This means that the pod will get scheduled only on a node that has a disktype=ssd label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23bb3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "spec:\n",
    "  affinity:\n",
    "    nodeAffinity:\n",
    "      requiredDuringSchedulingIgnoredDuringExecution:\n",
    "        nodeSelectorTerms:\n",
    "        - matchExpressions:\n",
    "          - key: disktype\n",
    "            operator: In\n",
    "            values:\n",
    "            - ssd            \n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "    imagePullPolicy: IfNotPresent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ae128ef",
   "metadata": {},
   "source": [
    "Apply the manifest to create a Pod that is scheduled onto your chosen node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32718067",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/pod-nginx-required-affinity.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bf7049d",
   "metadata": {},
   "source": [
    "Verify that the pod is running on your chosen node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89293654",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --output=wide"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba993bab",
   "metadata": {},
   "source": [
    "The output is similar to this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5cea696",
   "metadata": {},
   "source": [
    "NAME     READY     STATUS    RESTARTS   AGE    IP           NODE\n",
    "nginx    1/1       Running   0          13s    10.200.0.4   worker0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0aefb",
   "metadata": {},
   "source": [
    "##### Schedule a Pod using preferred node affinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bdcc039",
   "metadata": {},
   "source": [
    "This manifest describes a Pod that has a preferredDuringSchedulingIgnoredDuringExecution node affinity,disktype: ssd. This means that the pod will prefer a node that has a disktype=ssd label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "spec:\n",
    "  affinity:\n",
    "    nodeAffinity:\n",
    "      preferredDuringSchedulingIgnoredDuringExecution:\n",
    "      - weight: 1\n",
    "        preference:\n",
    "          matchExpressions:\n",
    "          - key: disktype\n",
    "            operator: In\n",
    "            values:\n",
    "            - ssd          \n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "    imagePullPolicy: IfNotPresent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c3ece67",
   "metadata": {},
   "source": [
    "Apply the manifest to create a Pod that is scheduled onto your chosen node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/pod-nginx-preferred-affinity.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78f6422b",
   "metadata": {},
   "source": [
    "Verify that the pod is running on your chosen node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --output=wide"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88949d37",
   "metadata": {},
   "source": [
    "The output is similar to this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46d1bc91",
   "metadata": {},
   "source": [
    "NAME     READY     STATUS    RESTARTS   AGE    IP           NODE\n",
    "nginx    1/1       Running   0          13s    10.200.0.4   worker0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0006a",
   "metadata": {},
   "source": [
    "### Node affinity weight "
   ]
  },
  {
   "cell_type": "raw",
   "id": "634e65d9",
   "metadata": {},
   "source": [
    "You can specify a weight between 1 and 100 for each instance of the preferredDuringSchedulingIgnoredDuringExecution affinity type. When the scheduler finds nodes that meet all the other scheduling requirements of the Pod, the scheduler iterates through every preferred rule that the node satisfies and adds the value of the weight for that expression to a sum.\n",
    "\n",
    "The final sum is added to the score of other priority functions for the node. Nodes with the highest total score are prioritized when the scheduler makes a scheduling decision for the Pod.\n",
    "\n",
    "For example, consider the following Pod spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: with-affinity-anti-affinity\n",
    "spec:\n",
    "  affinity:\n",
    "    nodeAffinity:\n",
    "      requiredDuringSchedulingIgnoredDuringExecution:\n",
    "        nodeSelectorTerms:\n",
    "        - matchExpressions:\n",
    "          - key: kubernetes.io/os\n",
    "            operator: In\n",
    "            values:\n",
    "            - linux\n",
    "      preferredDuringSchedulingIgnoredDuringExecution:\n",
    "      - weight: 1\n",
    "        preference:\n",
    "          matchExpressions:\n",
    "          - key: label-1\n",
    "            operator: In\n",
    "            values:\n",
    "            - key-1\n",
    "      - weight: 50\n",
    "        preference:\n",
    "          matchExpressions:\n",
    "          - key: label-2\n",
    "            operator: In\n",
    "            values:\n",
    "            - key-2\n",
    "  containers:\n",
    "  - name: with-node-affinity\n",
    "    image: registry.k8s.io/pause:2.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "292c8a03",
   "metadata": {},
   "source": [
    "If there are two possible nodes that match the preferredDuringSchedulingIgnoredDuringExecution rule, one with the label-1:key-1 label and another with the label-2:key-2 label, the scheduler considers the weight of each node and adds the weight to the other scores for that node, and schedules the Pod onto the node with the highest final score."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f611a316",
   "metadata": {},
   "source": [
    "Note: If you want Kubernetes to successfully schedule the Pods in this example, you must have existing nodes with the kubernetes.io/os=linux label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4887f6",
   "metadata": {},
   "source": [
    "### Node affinity per scheduling profile"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652ba8b7",
   "metadata": {},
   "source": [
    "When configuring multiple scheduling profiles, you can associate a profile with a node affinity, which is useful if a profile only applies to a specific set of nodes. To do so, add an addedAffinity to the args field of the NodeAffinity plugin in the scheduler configuration. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: kubescheduler.config.k8s.io/v1beta3\n",
    "kind: KubeSchedulerConfiguration\n",
    "\n",
    "profiles:\n",
    "  - schedulerName: default-scheduler\n",
    "  - schedulerName: foo-scheduler\n",
    "    pluginConfig:\n",
    "      - name: NodeAffinity\n",
    "        args:\n",
    "          addedAffinity:\n",
    "            requiredDuringSchedulingIgnoredDuringExecution:\n",
    "              nodeSelectorTerms:\n",
    "              - matchExpressions:\n",
    "                - key: scheduler-profile\n",
    "                  operator: In\n",
    "                  values:\n",
    "                  - foo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f294da6",
   "metadata": {},
   "source": [
    "The addedAffinity is applied to all Pods that set .spec.schedulerName to foo-scheduler, in addition to the NodeAffinity specified in the PodSpec. That is, in order to match the Pod, nodes need to satisfy addedAffinity and the Pod's .spec.NodeAffinity.\n",
    "\n",
    "Since the addedAffinity is not visible to end users, its behavior might be unexpected to them. Use node labels that have a clear correlation to the scheduler profile name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa17ed",
   "metadata": {},
   "source": [
    "### Inter-pod affinity and anti-affinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad81c06a",
   "metadata": {},
   "source": [
    "Inter-pod affinity and anti-affinity allow you to constrain which nodes your Pods can be scheduled on based on the labels of Pods already running on that node, instead of the node labels.\n",
    "\n",
    "Inter-pod affinity and anti-affinity rules take the form \"this Pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more Pods that meet rule Y\", where X is a topology domain like node, rack, cloud provider zone or region, or similar and Y is the rule Kubernetes tries to satisfy.\n",
    "\n",
    "You express these rules (Y) as label selectors with an optional associated list of namespaces. Pods are namespaced objects in Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors for Pod labels should specify the namespaces in which Kubernetes should look for those labels.\n",
    "\n",
    "You express the topology domain (X) using a topologyKey, which is the key for the node label that the system uses to denote the domain. For examples, see Well-Known Labels, Annotations and Taints."
   ]
  },
  {
   "cell_type": "raw",
   "id": "352ba6f6",
   "metadata": {},
   "source": [
    "Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d6930bb",
   "metadata": {},
   "source": [
    "Note: Pod anti-affinity requires nodes to be consistently labelled, in other words, every node in the cluster must have an appropriate label matching topologyKey. If some or all nodes are missing the specified topologyKey label, it can lead to unintended behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2b35d",
   "metadata": {},
   "source": [
    "### Types of inter-pod affinity and anti-affinity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "466285ee",
   "metadata": {},
   "source": [
    "Similar to node affinity are two types of Pod affinity and anti-affinity as follows:\n",
    "\n",
    "1. requiredDuringSchedulingIgnoredDuringExecution\n",
    "2. preferredDuringSchedulingIgnoredDuringExecution\n",
    "\n",
    "For example, you could use requiredDuringSchedulingIgnoredDuringExecution affinity to tell the scheduler to co-locate Pods of two services in the same cloud provider zone because they communicate with each other a lot. Similarly, you could use preferredDuringSchedulingIgnoredDuringExecution anti-affinity to spread Pods from a service across multiple cloud provider zones.\n",
    "\n",
    "To use inter-pod affinity, use the affinity.podAffinity field in the Pod spec. For inter-pod anti-affinity, use the affinity.podAntiAffinity field in the Pod spec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b21860",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60a463f3",
   "metadata": {},
   "source": [
    "Consider the following Pod spec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: with-pod-affinity\n",
    "spec:\n",
    "  affinity:\n",
    "    podAffinity:\n",
    "      requiredDuringSchedulingIgnoredDuringExecution:\n",
    "      - labelSelector:\n",
    "          matchExpressions:\n",
    "          - key: security\n",
    "            operator: In\n",
    "            values:\n",
    "            - S1\n",
    "        topologyKey: topology.kubernetes.io/zone\n",
    "    podAntiAffinity:\n",
    "      preferredDuringSchedulingIgnoredDuringExecution:\n",
    "      - weight: 100\n",
    "        podAffinityTerm:\n",
    "          labelSelector:\n",
    "            matchExpressions:\n",
    "            - key: security\n",
    "              operator: In\n",
    "              values:\n",
    "              - S2\n",
    "          topologyKey: topology.kubernetes.io/zone\n",
    "  containers:\n",
    "  - name: with-pod-affinity\n",
    "    image: registry.k8s.io/pause:2.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1f7e421",
   "metadata": {},
   "source": [
    "This example defines one Pod affinity rule and one Pod anti-affinity rule. The Pod affinity rule uses the \"hard\" requiredDuringSchedulingIgnoredDuringExecution, while the anti-affinity rule uses the \"soft\" preferredDuringSchedulingIgnoredDuringExecution.\n",
    "\n",
    "The affinity rule says that the scheduler can only schedule a Pod onto a node if the node is in the same zone as one or more existing Pods with the label security=S1. More precisely, the scheduler must place the Pod on a node that has the topology.kubernetes.io/zone=V label, as long as there is at least one node in that zone that currently has one or more Pods with the Pod label security=S1.\n",
    "\n",
    "The anti-affinity rule says that the scheduler should try to avoid scheduling the Pod onto a node that is in the same zone as one or more Pods with the label security=S2. More precisely, the scheduler should try to avoid placing the Pod on a node that has the topology.kubernetes.io/zone=R label if there are other nodes in the same zone currently running Pods with the Security=S2 Pod label."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2657ce8a",
   "metadata": {},
   "source": [
    "To get yourself more familiar with the examples of Pod affinity and anti-affinity, refer to the design proposal.\n",
    "\n",
    "You can use the In, NotIn, Exists and DoesNotExist values in the operator field for Pod affinity and anti-affinity."
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e3ca34b",
   "metadata": {},
   "source": [
    "In principle, the topologyKey can be any allowed label key with the following exceptions for performance and security reasons:\n",
    "\n",
    "1. For Pod affinity and anti-affinity, an empty topologyKey field is not allowed in both \n",
    "   requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.\n",
    "2. For requiredDuringSchedulingIgnoredDuringExecution Pod anti-affinity rules, the admission controller \n",
    "   LimitPodHardAntiAffinityTopology limits topologyKey to kubernetes.io/hostname. You can modify or disable the admission \n",
    "   controller if you want to allow custom topologies."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9092bd37",
   "metadata": {},
   "source": [
    "In addition to labelSelector and topologyKey, you can optionally specify a list of namespaces which the labelSelector should match against using the namespaces field at the same level as labelSelector and topologyKey. If omitted or empty, namespaces defaults to the namespace of the Pod where the affinity/anti-affinity definition appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd2f4a",
   "metadata": {},
   "source": [
    "### nodeName"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4693bd37",
   "metadata": {},
   "source": [
    "nodeName is a more direct form of node selection than affinity or nodeSelector. nodeName is a field in the Pod spec. If the nodeName field is not empty, the scheduler ignores the Pod and the kubelet on the named node tries to place the Pod on that node. Using nodeName overrules using nodeSelector or affinity and anti-affinity rules."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ebaa733",
   "metadata": {},
   "source": [
    "Some of the limitations of using nodeName to select nodes are:\n",
    "\n",
    "1. If the named node does not exist, the Pod will not run, and in some cases may be automatically deleted.\n",
    "2. If the named node does not have the resources to accommodate the Pod, the Pod will fail and its reason will indicate \n",
    "   why, for example OutOfmemory or OutOfcpu.\n",
    "3. Node names in cloud environments are not always predictable or stable."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3c1ff29",
   "metadata": {},
   "source": [
    "Here is an example of a Pod spec using the nodeName field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f25224",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "spec:\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "  nodeName: kube-01"
   ]
  },
  {
   "cell_type": "raw",
   "id": "214fcbc7",
   "metadata": {},
   "source": [
    "The above Pod will only run on the node kube-01."
   ]
  },
  {
   "cell_type": "raw",
   "id": "02eb51bd",
   "metadata": {},
   "source": [
    "注意: \n",
    "\n",
    "1. taint 的 node 是否可以接受這種 pod？ 答案是 yes\n",
    "2. cordon 的 node 是否可以接受这种 pod？ 答案是 yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e777b",
   "metadata": {},
   "source": [
    "### Pod topology spread constraints"
   ]
  },
  {
   "cell_type": "raw",
   "id": "604b6354",
   "metadata": {},
   "source": [
    "You can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other topology domains that you define. You might do this to improve performance, expected availability, or overall utilization.\n",
    "\n",
    "Read Pod topology spread constraints to learn more about how these work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ce81d",
   "metadata": {},
   "source": [
    "### Taints and Tolerations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3bceccb",
   "metadata": {},
   "source": [
    "Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods.\n",
    "\n",
    "Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.\n",
    "\n",
    "Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2bcff",
   "metadata": {},
   "source": [
    "##### Concepts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fca1d477",
   "metadata": {},
   "source": [
    "You add a taint to a node using kubectl taint. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl taint nodes node1 key1=value1:NoSchedule"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57f3e86b",
   "metadata": {},
   "source": [
    "places a taint on node node1. The taint has key key1, value value1, and taint effect NoSchedule. This means that no pod will be able to schedule onto node1 unless it has a matching toleration.\n",
    "\n",
    "To remove the taint added by the command above, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl taint nodes node1 key1=value1:NoSchedule-"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48486714",
   "metadata": {},
   "source": [
    "You specify a toleration for a pod in the PodSpec. Both of the following tolerations \"match\" the taint created by the kubectl taint line above, and thus a pod with either toleration would be able to schedule onto node1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b975560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerations:\n",
    "- key: \"key1\"\n",
    "  operator: \"Equal\"\n",
    "  value: \"value1\"\n",
    "  effect: \"NoSchedule\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d78231",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerations:\n",
    "- key: \"key1\"\n",
    "  operator: \"Exists\"\n",
    "  effect: \"NoSchedule\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e5579a9",
   "metadata": {},
   "source": [
    "Here's an example of a pod that uses tolerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6538b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "  labels:\n",
    "    env: test\n",
    "spec:\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "    imagePullPolicy: IfNotPresent\n",
    "  tolerations:\n",
    "  - key: \"example-key\"\n",
    "    operator: \"Exists\"\n",
    "    effect: \"NoSchedule\"`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fdf030a",
   "metadata": {},
   "source": [
    "The default value for operator is Equal.\n",
    "\n",
    "A toleration \"matches\" a taint if the keys are the same and the effects are the same, and:\n",
    "\n",
    "1. the operator is Exists (in which case no value should be specified), or\n",
    "2. the operator is Equal and the values are equal."
   ]
  },
  {
   "cell_type": "raw",
   "id": "53f06117",
   "metadata": {},
   "source": [
    "Note:\n",
    "There are two special cases:\n",
    "\n",
    "An empty key with operator Exists matches all keys, values and effects which means this will tolerate everything.\n",
    "An empty effect matches all effects with key key1."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6c464b2",
   "metadata": {},
   "source": [
    "The above example used effect of NoSchedule. Alternatively, you can use effect of PreferNoSchedule. This is a \"preference\" or \"soft\" version of NoSchedule -- the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not required. The third kind of effect is NoExecute, described later.\n",
    "\n",
    "You can put multiple taints on the same node and multiple tolerations on the same pod. The way Kubernetes processes multiple taints and tolerations is like a filter: start with all of a node's taints, then ignore the ones for which the pod has a matching toleration; the remaining un-ignored taints have the indicated effects on the pod. In particular,\n",
    "\n",
    "1. if there is at least one un-ignored taint with effect NoSchedule then Kubernetes will not schedule the pod onto that \n",
    "   node\n",
    "2. if there is no un-ignored taint with effect NoSchedule but there is at least one un-ignored taint with effect \n",
    "   PreferNoSchedule then Kubernetes will try to not schedule the pod onto the node\n",
    "3. if there is at least one un-ignored taint with effect NoExecute then the pod will be evicted from the node (if it is  \n",
    "   already running on the node), and will not be scheduled onto the node (if it is not yet running on the node)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a593f18",
   "metadata": {},
   "source": [
    "For example, imagine you taint a node like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl taint nodes node1 key1=value1:NoSchedule\n",
    "kubectl taint nodes node1 key1=value1:NoExecute\n",
    "kubectl taint nodes node1 key2=value2:NoSchedule"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa439978",
   "metadata": {},
   "source": [
    "And a pod has two tolerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerations:\n",
    "- key: \"key1\"\n",
    "  operator: \"Equal\"\n",
    "  value: \"value1\"\n",
    "  effect: \"NoSchedule\"\n",
    "- key: \"key1\"\n",
    "  operator: \"Equal\"\n",
    "  value: \"value1\"\n",
    "  effect: \"NoExecute\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e73e638",
   "metadata": {},
   "source": [
    "In this case, the pod will not be able to schedule onto the node, because there is no toleration matching the third taint. But it will be able to continue running if it is already running on the node when the taint is added, because the third taint is the only one of the three that is not tolerated by the pod.\n",
    "\n",
    "Normally, if a taint with effect NoExecute is added to a node, then any pods that do not tolerate the taint will be evicted immediately, and pods that do tolerate the taint will never be evicted. However, a toleration with NoExecute effect can specify an optional tolerationSeconds field that dictates how long the pod will stay bound to the node after the taint is added. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerations:\n",
    "- key: \"key1\"\n",
    "  operator: \"Equal\"\n",
    "  value: \"value1\"\n",
    "  effect: \"NoExecute\"\n",
    "  tolerationSeconds: 3600"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bd8315a",
   "metadata": {},
   "source": [
    "means that if this pod is running and a matching taint is added to the node, then the pod will stay bound to the node for 3600 seconds, and then be evicted. If the taint is removed before that time, the pod will not be evicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe5892",
   "metadata": {},
   "source": [
    "# Cordoning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f920435c",
   "metadata": {},
   "source": [
    "Cordoning 是把一個節點標記為 unschedulabel， 一旦標記後，就不會有新的 pod 被部署到這個節點上了。 但已經運行在這個節點的 pod 不受影響。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94164e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl cordon <node_name>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21caab0c",
   "metadata": {},
   "source": [
    "當我們要維護一個節點時，一般會通過 cordon 標記這個節點。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18f106fd",
   "metadata": {},
   "source": [
    "維護結束後可以重新標記一個節點為 schedulable。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl uncordon <node_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33111ea1",
   "metadata": {},
   "source": [
    "# drain"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b655ad2",
   "metadata": {},
   "source": [
    "drain 可以 gracefully 的停止一個節點上的 Pod。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41220ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl drain <node name> --ingore-daemonsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff750889",
   "metadata": {},
   "source": [
    "### 範例: Safely Drain a Node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71c87211",
   "metadata": {},
   "source": [
    "This example shows how to safely drain a node, optionally respecting the PodDisruptionBudget you have defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc1e1e",
   "metadata": {},
   "source": [
    "##### Before you begin"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd6e4e79",
   "metadata": {},
   "source": [
    "Your Kubernetes server must be at or later than version 1.5. To check the version, enter kubectl version.\n",
    "\n",
    "This task also assumes that you have met the following prerequisites:\n",
    "\n",
    "1. You do not require your applications to be highly available during the node drain, or\n",
    "2. You have read about the PodDisruptionBudget concept, and have configured PodDisruptionBudgets for applications that need \n",
    "   them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b4f613",
   "metadata": {},
   "source": [
    "##### (Optional) Configure a disruption budget"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f1de428",
   "metadata": {},
   "source": [
    "To ensure that your workloads remain available during maintenance, you can configure a PodDisruptionBudget.\n",
    "\n",
    "If availability is important for any applications that run or could run on the node(s) that you are draining, configure a PodDisruptionBudgets first and then continue following this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48f3e8",
   "metadata": {},
   "source": [
    "##### Use kubectl drain to remove a node from service"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8042eff9",
   "metadata": {},
   "source": [
    "You can use kubectl drain to safely evict all of your pods from a node before you perform maintenance on the node (e.g. kernel upgrade, hardware maintenance, etc.). Safe evictions allow the pod's containers to gracefully terminate and will respect the PodDisruptionBudgets you have specified."
   ]
  },
  {
   "cell_type": "raw",
   "id": "30f7a007",
   "metadata": {},
   "source": [
    "Note: By default kubectl drain ignores certain system pods on the node that cannot be killed; see the kubectl drain documentation for more details."
   ]
  },
  {
   "cell_type": "raw",
   "id": "31877d68",
   "metadata": {},
   "source": [
    "When kubectl drain returns successfully, that indicates that all of the pods (except the ones excluded as described in the previous paragraph) have been safely evicted (respecting the desired graceful termination period, and respecting the PodDisruptionBudget you have defined). It is then safe to bring down the node by powering down its physical machine or, if running on a cloud platform, deleting its virtual machine."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2475660",
   "metadata": {},
   "source": [
    "First, identify the name of the node you wish to drain. You can list all of the nodes in your cluster with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec66124",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get nodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2ed22d3",
   "metadata": {},
   "source": [
    "Next, tell Kubernetes to drain the node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585b0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl drain <node name>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77a672d2",
   "metadata": {},
   "source": [
    "Once it returns (without giving an error), you can power down the node (or equivalently, if on a cloud platform, delete the virtual machine backing the node). If you leave the node in the cluster during the maintenance operation, you need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl uncordon <node name>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b16b835e",
   "metadata": {},
   "source": [
    "afterwards to tell Kubernetes that it can resume scheduling new pods onto the node."
   ]
  },
  {
   "cell_type": "raw",
   "id": "49c5fdd8",
   "metadata": {},
   "source": [
    "注意只要是 drain 了一個 node，它就會變成 unshedulable 是 true 了。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02394c48",
   "metadata": {},
   "source": [
    "##### Draining multiple nodes in parallel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4e37d78",
   "metadata": {},
   "source": [
    "The kubectl drain command should only be issued to a single node at a time. However, you can run multiple kubectl drain commands for different nodes in parallel, in different terminals or in the background. Multiple drain commands running concurrently will still respect the PodDisruptionBudget you specify.\n",
    "\n",
    "For example, if you have a StatefulSet with three replicas and have set a PodDisruptionBudget for that set specifying minAvailable: 2, kubectl drain only evicts a pod from the StatefulSet if all three replicas pods are ready; if then you issue multiple drain commands in parallel, Kubernetes respects the PodDisruptionBudget and ensure that only 1 (calculated as replicas - minAvailable) Pod is unavailable at any given time. Any drains that would cause the number of ready replicas to fall below the specified budget are blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fefcaff",
   "metadata": {},
   "source": [
    "##### The Eviction API"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca7f486f",
   "metadata": {},
   "source": [
    "If you prefer not to use kubectl drain (such as to avoid calling to an external command, or to get finer control over the pod eviction process), you can also programmatically cause evictions using the eviction API.\n",
    "\n",
    "For more information, see API-initiated eviction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
