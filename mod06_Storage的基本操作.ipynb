{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de5a43f",
   "metadata": {},
   "source": [
    "# Volumes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdc1e515",
   "metadata": {},
   "source": [
    "On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. One problem is the loss of files when a container crashes. The kubelet restarts the container but with a clean state. A second problem occurs when sharing files between containers running together in a Pod. The Kubernetes volume abstraction solves both of these problems. Familiarity with Pods is suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a9a39",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcb5d32f",
   "metadata": {},
   "source": [
    "Docker has a concept of volumes, though it is somewhat looser and less managed. A Docker volume is a directory on disk or in another container. Docker provides volume drivers, but the functionality is somewhat limited.\n",
    "\n",
    "Kubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously. Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes; however, Kubernetes does not destroy persistent volumes. For any kind of volume in a given pod, data is preserved across container restarts.\n",
    "\n",
    "At its core, a volume is a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.\n",
    "\n",
    "To use a volume, specify the volumes to provide for the Pod in .spec.volumes and declare where to mount those volumes into containers in .spec.containers[*].volumeMounts. A process in a container sees a filesystem view composed from the initial contents of the container image, plus volumes (if defined) mounted inside the container. The process sees a root filesystem that initially matches the contents of the container image. Any writes to within that filesystem hierarchy, if allowed, affect what that process views when it performs a subsequent filesystem access. Volumes mount at the specified paths within the image. For each container defined within a Pod, you must independently specify where to mount each volume that the container uses.\n",
    "\n",
    "Volumes cannot mount within other volumes (but see Using subPath for a related mechanism). Also, a volume cannot contain a hard link to anything in a different volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad9625",
   "metadata": {},
   "source": [
    "# Types of volumes "
   ]
  },
  {
   "cell_type": "raw",
   "id": "14177068",
   "metadata": {},
   "source": [
    "Kubernetes supports several types of volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1c06a",
   "metadata": {},
   "source": [
    "### emptyDir"
   ]
  },
  {
   "cell_type": "raw",
   "id": "185b2082",
   "metadata": {},
   "source": [
    "An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. As the name says, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently."
   ]
  },
  {
   "cell_type": "raw",
   "id": "376eed47",
   "metadata": {},
   "source": [
    "Note: A container crashing does not remove a Pod from a node. The data in an emptyDir volume is safe across container crashes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "73f78f8a",
   "metadata": {},
   "source": [
    "Some uses for an emptyDir are:\n",
    "\n",
    "scratch space, such as for a disk-based merge sort\n",
    "checkpointing a long computation for recovery from crashes\n",
    "holding files that a content-manager container fetches while a webserver container serves the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ca2d152",
   "metadata": {},
   "source": [
    "Depending on your environment, emptyDir volumes are stored on whatever medium that backs the node such as disk or SSD, or network storage. However, if you set the emptyDir.medium field to \"Memory\", Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on node reboot and any files you write count against your container's memory limit."
   ]
  },
  {
   "cell_type": "raw",
   "id": "43c9ad73",
   "metadata": {},
   "source": [
    "Note: If the SizeMemoryBackedVolumes feature gate is enabled, you can specify a size for memory backed volumes. If no size is specified, memory backed volumes are sized to 50% of the memory on a Linux host."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bde82507",
   "metadata": {},
   "source": [
    "emptyDir configuration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-pd\n",
    "spec:\n",
    "  containers:\n",
    "  - image: registry.k8s.io/test-webserver\n",
    "    name: test-container\n",
    "    volumeMounts:\n",
    "    - mountPath: /cache\n",
    "      name: cache-volume\n",
    "  volumes:\n",
    "  - name: cache-volume\n",
    "    emptyDir: {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334fe29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89e6071e",
   "metadata": {},
   "source": [
    "每當我們建立一個新的 Pod 物件時，Kubernetes 就會在這個 Pod 裏建立一個 emptyDir，該 Pod 中所有的 container 都可以讀寫 emptyDir 中的資料。當 Pod 從 Node 中被移除時，emptyDir 也會隨之消失，emptyDir 有以下幾個用途："
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a831a72",
   "metadata": {},
   "source": [
    "1. 暫時性儲存空間\n",
    "   例如某些應用程式運行時需要一些臨時而無需永久保存的資料夾\n",
    "2. 共用儲存空間\n",
    "   正如上述提到，同一個 Pod 中所有的 containers 都可以讀寫 emptyDir，也可以將 emptyDir 當作是這些 containers 的共用目錄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651dcc2",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818281d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: multicontainer-pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: producer\n",
    "    image: busybox\n",
    "    command: [\"sh\", \"-c\", \"while true; do echo $(hostname) $(date) >> /var/log/index.html; sleep 10; done\"]\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /var/log\n",
    "  - name: consumer\n",
    "    image: nginx\n",
    "    ports:\n",
    "      - containerPort: 80\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /usr/share/nginx/html\n",
    "  volumes:\n",
    "  - name: webcontent\n",
    "    emptyDir: {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1895e26",
   "metadata": {},
   "source": [
    "### hostPath"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48d4a551",
   "metadata": {},
   "source": [
    "Warning:\n",
    "HostPath volumes present many security risks, and it is a best practice to avoid the use of HostPaths when possible. When a HostPath volume must be used, it should be scoped to only the required file or directory, and mounted as ReadOnly.\n",
    "\n",
    "If restricting HostPath access to specific directories through AdmissionPolicy, volumeMounts MUST be required to use readOnly mounts for the policy to be effective."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f8ceaee",
   "metadata": {},
   "source": [
    "A hostPath volume mounts a file or directory from the host node's filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch(緊急出口) for some applications.\n",
    "\n",
    "For example, some uses for a hostPath are:\n",
    "\n",
    "running a container that needs access to Docker internals; use a hostPath of /var/lib/docker\n",
    "running cAdvisor in a container; use a hostPath of /sys\n",
    "allowing a Pod to specify whether a given hostPath should exist prior to the Pod running, whether it should be created, and what it should exist as"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56366590",
   "metadata": {},
   "source": [
    "In addition to the required path property, you can optionally specify a type for a hostPath volume."
   ]
  },
  {
   "cell_type": "raw",
   "id": "029d6ac0",
   "metadata": {},
   "source": [
    "The supported values for field type are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f38f76",
   "metadata": {},
   "source": [
    "<img src='./img/30.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63c7da5c",
   "metadata": {},
   "source": [
    "Watch out when using this type of volume, because:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a7f46d4",
   "metadata": {},
   "source": [
    "1. HostPaths can expose privileged system credentials (such as for the Kubelet) or privileged APIs (such as container r\n",
    "   untime socket), which can be used for container escape or to attack other parts of the cluster.\n",
    "2. Pods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to \n",
    "   different files on the nodes\n",
    "3. The files or directories created on the underlying hosts are only writable by root. You either need to run your process \n",
    "   as root in a privileged Container or modify the file permissions on the host to be able to write to a hostPath volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc5da52c",
   "metadata": {},
   "source": [
    "hostPath configuration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e55ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-pd\n",
    "spec:\n",
    "  containers:\n",
    "  - image: registry.k8s.io/test-webserver\n",
    "    name: test-container\n",
    "    volumeMounts:\n",
    "    - mountPath: /test-pd\n",
    "      name: test-volume\n",
    "  volumes:\n",
    "  - name: test-volume\n",
    "    hostPath:\n",
    "      # directory location on host\n",
    "      path: /data\n",
    "      # this field is optional\n",
    "      type: Directory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2828e8a",
   "metadata": {},
   "source": [
    "Caution: The FileOrCreate mode does not create the parent directory of the file. If the parent directory of the mounted file does not exist, the pod fails to start. To ensure that this mode works, you can try to mount directories and files separately, as shown in the FileOrCreateconfiguration."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2e70be9",
   "metadata": {},
   "source": [
    "hostPath FileOrCreate configuration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-webserver\n",
    "spec:\n",
    "  containers:\n",
    "  - name: test-webserver\n",
    "    image: registry.k8s.io/test-webserver:latest\n",
    "    volumeMounts:\n",
    "    - mountPath: /var/local/aaa\n",
    "      name: mydir\n",
    "    - mountPath: /var/local/aaa/1.txt\n",
    "      name: myfile\n",
    "  volumes:\n",
    "  - name: mydir\n",
    "    hostPath:\n",
    "      # Ensure the file directory is created.\n",
    "      path: /var/local/aaa\n",
    "      type: DirectoryOrCreate\n",
    "  - name: myfile\n",
    "    hostPath:\n",
    "      path: /var/local/aaa/1.txt\n",
    "      type: FileOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3c6ed",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f5fb032",
   "metadata": {},
   "source": [
    "注意我們的 minikube 是容器，所以要進去 minikube 容器裡面看 /tmp 是否有同步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: multicontainer-pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: producer\n",
    "    image: busybox\n",
    "    command: [\"sh\", \"-c\", \"while true; do echo $(hostname) $(date) >> /var/log/index.html; sleep 10; done\"]\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /var/log\n",
    "  - name: consumer\n",
    "    image: nginx\n",
    "    ports:\n",
    "      - containerPort: 80\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /usr/share/nginx/html\n",
    "  volumes:\n",
    "  - name: webcontent\n",
    "    hostPath:\n",
    "      path: /tmp\n",
    "      type: Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba89d7a",
   "metadata": {},
   "source": [
    "### local"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d73c3846",
   "metadata": {},
   "source": [
    "A local volume represents a mounted local storage device such as a disk, partition or directory.\n",
    "\n",
    "Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.\n",
    "\n",
    "Compared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to nodes. The system is aware of the volume's node constraints by looking at the node affinity on the PersistentVolume.\n",
    "\n",
    "However, local volumes are subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.\n",
    "\n",
    "The following example shows a PersistentVolume using a local volume and nodeAffinity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80922b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: example-pv\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 100Gi\n",
    "  volumeMode: Filesystem\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  persistentVolumeReclaimPolicy: Delete\n",
    "  storageClassName: local-storage\n",
    "  local:\n",
    "    path: /mnt/disks/ssd1\n",
    "  nodeAffinity:\n",
    "    required:\n",
    "      nodeSelectorTerms:\n",
    "      - matchExpressions:\n",
    "        - key: kubernetes.io/hostname\n",
    "          operator: In\n",
    "          values:\n",
    "          - example-node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46a23ec8",
   "metadata": {},
   "source": [
    "You must set a PersistentVolume nodeAffinity when using local volumes. The Kubernetes scheduler uses the PersistentVolume nodeAffinity to schedule these Pods to the correct node.\n",
    "\n",
    "PersistentVolume volumeMode can be set to \"Block\" (instead of the default value \"Filesystem\") to expose the local volume as a raw block device.\n",
    "\n",
    "When using local volumes, it is recommended to create a StorageClass with volumeBindingMode set to WaitForFirstConsumer. For more details, see the local StorageClass example. Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have, such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.\n",
    "\n",
    "An external static provisioner can be run separately for improved management of the local volume lifecycle. Note that this provisioner does not support dynamic provisioning yet. For an example on how to run an external local provisioner, see the local volume provisioner user guide."
   ]
  },
  {
   "cell_type": "raw",
   "id": "40c44276",
   "metadata": {},
   "source": [
    "Note: The local PersistentVolume requires manual cleanup and deletion by the user if the external static provisioner is not used to manage the volume lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73946b",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d47f22",
   "metadata": {},
   "source": [
    "參考文章: https://www.cnblogs.com/rexcheny/p/10925464.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a3e9078",
   "metadata": {},
   "source": [
    "本地持久化存儲（Local Persistent Volume）就是把數據存儲在 POD 運行的宿主機上，我們知道宿主機有 hostPath 和 emptyDir，由於這兩種的特性不適用於本地持久化存儲。那麽本地持久化存儲必須能保證 POD 被調度到具有本地持久化存儲的節點上。\n",
    "\n",
    "為什麽需要這種類型的存儲呢？有時候你的應用對磁盤 IO 有很高的要求，網絡存儲性能肯定不如本地的高，尤其是本地使用了 SSD 這種磁盤。\n",
    "\n",
    "但這裏有個問題，通常我們先創建 PV，然後創建 PVC，這時候如果兩者匹配那麽系統會自動進行綁定，哪怕是動態 PV 創建，也是先調度 POD 到任意一個節點，然後根據 PVC 來進行創建 P V然後進行綁定最後掛載到 POD 中，可是本地持久化存儲有一個問題就是這種 PV 必須要先準備好，而且不一定集群所有節點都有這種 PV，如果 POD 隨意調度肯定不行，如何保證 POD 一定會被調度到有 PV 的節點上呢？這時候就需要在 PV 中聲明節點親和，且POD 被調度的時候還要考慮卷的分布情況。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88aadc37",
   "metadata": {},
   "source": [
    "定義 PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68255f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: example-pv\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 5Gi\n",
    "  volumeMode: Filesystem\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  persistentVolumeReclaimPolicy: Delete\n",
    "  storageClassName: local-storage\n",
    "  local: # local類型\n",
    "    path: /data/vol1  # 節點上的具體路徑\n",
    "  nodeAffinity: # 這裏就設置了節點親和\n",
    "    required:\n",
    "      nodeSelectorTerms:\n",
    "      - matchExpressions:\n",
    "        - key: kubernetes.io/hostname\n",
    "          operator: In\n",
    "          values:\n",
    "          - node01 # 這裏我們使用 node01 節點，該節點有 /data/vol1 路徑"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0affcf0a",
   "metadata": {},
   "source": [
    "如果你在 node02 上也有 /data/vol1 這個目錄，上面這個 PV 也一定不會在 node02 上，因為下面的 nodeAffinity 設置了主機名就等於 node01。另外這種本地 PV 通常推薦使用的是宿主機上單獨的硬盤設備，而不是和操作系統共有一塊硬盤，雖然可以這樣用。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0da6ff3e",
   "metadata": {},
   "source": [
    "定義 storageClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind: StorageClass\n",
    "apiVersion: storage.k8s.io/v1\n",
    "metadata:\n",
    "  name: local-storage\n",
    "provisioner: kubernetes.io/no-provisioner\n",
    "volumeBindingMode: WaitForFirstConsumer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb48ef27",
   "metadata": {},
   "source": [
    "這裏的 volumeBindingMode: WaitForFirstConsumer 很關鍵，意思就是延遲綁定，當有符合 PVC 要求的 PV 不立即綁定。因為 POD 使用 PVC，而綁定之後，POD 被調度到其他節點，顯然其他節點很有可能沒有那個 PV 所以 POD 就掛起了，另外就算該節點有合適的 PV，而 POD 被設置成不能運行在該節點，這時候就沒法了，延遲綁定的好處是，POD 的調度要參考卷的分布。當開始調度 POD 的時候看看它要求的 LPV 在哪裏，然後就調度到該節點，然後進行 PVC 的綁定，最後在掛載到 POD 中，這樣就保證了 POD 所在的節點就一定是 LPV 所在的節點。所以讓 PVC 延遲綁定，就是等到使用這個 PVC 的 POD 出現在調度器上之後（真正被調度之前），然後根據綜合評估再來綁定這個 PVC。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "065dce73",
   "metadata": {},
   "source": [
    "定義 PVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e3806",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind: PersistentVolumeClaim\n",
    "apiVersion: v1\n",
    "metadata:\n",
    "  name: local-claim\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 5Gi\n",
    "  storageClassName: local-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d3c03",
   "metadata": {},
   "source": [
    "<img src='./img/33.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6650ecf",
   "metadata": {},
   "source": [
    "可以看到這個 PVC 是 pending 狀態，這也就是延遲綁定，因為此時還沒有 POD。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39e52965",
   "metadata": {},
   "source": [
    "定義 POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: tomcat-deploy\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      appname: myapp\n",
    "  template:\n",
    "    metadata:\n",
    "      name: myapp\n",
    "      labels:\n",
    "        appname: myapp\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: myapp\n",
    "        image: tomcat:8.5.38-jre8\n",
    "        ports:\n",
    "        - name: http\n",
    "          containerPort: 8080\n",
    "          protocol: TCP\n",
    "        volumeMounts:\n",
    "          - name: tomcatedata\n",
    "            mountPath : \"/data\"\n",
    "      volumes:\n",
    "        - name: tomcatedata\n",
    "          persistentVolumeClaim:\n",
    "            claimName: local-claim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bce125",
   "metadata": {},
   "source": [
    "<img src='./img/34.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22815b91",
   "metadata": {},
   "source": [
    "這個 POD 被調度到 node01 上，因為我們的 PV 就在 node01 上，這時候你刪除這個 POD，然後在重建該 POD，那麽依然會被調度到 node01 上。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c101e18b",
   "metadata": {},
   "source": [
    "總結：本地卷也就是 LPV 不支持動態供給的方式，延遲綁定，就是為了綜合考慮所有因素再進行 POD 調度。其根本原因是動態供給是先調度 POD 到節點，然後動態創建 PV 以及綁定 PVC 最後運行 POD；而 LPV 是先創建與某一節點關聯的 PV，然後在調度的時候綜合考慮各種因素而且要包括 PV 在哪個節點，然後再進行調度，到達該節點後在進行 PVC 的綁定。也就說動態供給不考慮節點，LPV 必須考慮節點。所以這兩種機制有沖突導致無法在動態供給策略下使用 LPV。換句話說動態供給是 PV 跟著 POD 走，而 LPV 是 POD 跟著 PV 走。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f16c1",
   "metadata": {},
   "source": [
    "### nfs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "361516c4",
   "metadata": {},
   "source": [
    "An nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be shared between pods. NFS can be mounted by multiple writers simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-pd\n",
    "spec:\n",
    "  containers:\n",
    "  - image: registry.k8s.io/test-webserver\n",
    "    name: test-container\n",
    "    volumeMounts:\n",
    "    - mountPath: /my-nfs-data\n",
    "      name: test-volume\n",
    "  volumes:\n",
    "  - name: test-volume\n",
    "    nfs:\n",
    "      server: my-nfs-server.example.com\n",
    "      path: /my-nfs-volume\n",
    "      readOnly: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8da4c5",
   "metadata": {},
   "source": [
    "### configMap"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19eaae6e",
   "metadata": {},
   "source": [
    "A ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of type configMap and then consumed by containerized applications running in a pod.\n",
    "\n",
    "When referencing a ConfigMap, you provide the name of the ConfigMap in the volume. You can customize the path to use for a specific entry in the ConfigMap. The following configuration shows how to mount the log-config ConfigMap onto a Pod called configmap-pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d131a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: configmap-pod\n",
    "spec:\n",
    "  containers:\n",
    "    - name: test\n",
    "      image: busybox:1.28\n",
    "      volumeMounts:\n",
    "        - name: config-vol\n",
    "          mountPath: /etc/config\n",
    "  volumes:\n",
    "    - name: config-vol\n",
    "      configMap:\n",
    "        name: log-config\n",
    "        items:\n",
    "          - key: log_level\n",
    "            path: log_level"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce2cf0bc",
   "metadata": {},
   "source": [
    "The log-config ConfigMap is mounted as a volume, and all contents stored in its log_level entry are mounted into the Pod at path /etc/config/log_level. Note that this path is derived from the volume's mountPath and the path keyed with log_level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b0459",
   "metadata": {},
   "source": [
    "### 範例: Configuring Redis using a ConfigMap"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1083e99a",
   "metadata": {},
   "source": [
    "This page provides a real world example of how to configure Redis using a ConfigMap and builds upon the Configure a Pod to Use a ConfigMap task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d154f",
   "metadata": {},
   "source": [
    "##### Objectives"
   ]
  },
  {
   "cell_type": "raw",
   "id": "181fa90d",
   "metadata": {},
   "source": [
    "Create a ConfigMap with Redis configuration values\n",
    "Create a Redis Pod that mounts and uses the created ConfigMap\n",
    "Verify that the configuration was correctly applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca2541",
   "metadata": {},
   "source": [
    "##### Before you begin"
   ]
  },
  {
   "cell_type": "raw",
   "id": "131bbc20",
   "metadata": {},
   "source": [
    "You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:\n",
    "\n",
    "- Killercoda\n",
    "- Play with Kubernetes\n",
    "\n",
    "To check the version, enter kubectl version.\n",
    "\n",
    "- The example shown on this page works with kubectl 1.14 and above.\n",
    "- Understand Configure a Pod to Use a ConfigMap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74bca02",
   "metadata": {},
   "source": [
    "##### Real World Example"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e5edd3b",
   "metadata": {},
   "source": [
    "Follow the steps below to configure a Redis cache using data stored in a ConfigMap.\n",
    "First create a ConfigMap with an empty configuration block:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19165da3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6041f90b",
   "metadata": {},
   "source": [
    "example-redis-config.yaml 檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8488c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: example-redis-config\n",
    "data:\n",
    "  redis-config: \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1921f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a44b53f",
   "metadata": {},
   "source": [
    "redis.yaml 檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  containers:\n",
    "  - name: redis\n",
    "    image: redis:5.0.4\n",
    "    command:\n",
    "      - redis-server\n",
    "      - \"/redis-master/redis.conf\"\n",
    "    env:\n",
    "    - name: MASTER\n",
    "      value: \"true\"\n",
    "    ports:\n",
    "    - containerPort: 6379\n",
    "    resources:\n",
    "      limits:\n",
    "        cpu: \"0.1\"\n",
    "    volumeMounts:\n",
    "    - mountPath: /redis-master-data\n",
    "      name: data\n",
    "    - mountPath: /redis-master\n",
    "      name: config\n",
    "  volumes:\n",
    "    - name: data\n",
    "      emptyDir: {}\n",
    "    - name: config\n",
    "      configMap:\n",
    "        name: example-redis-config\n",
    "        items:\n",
    "        - key: redis-config\n",
    "          path: redis.conf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48fab94d",
   "metadata": {},
   "source": [
    "Examine the contents of the Redis pod manifest and note the following:\n",
    "\n",
    "1. A volume named config is created by spec.volumes[1]\n",
    "2. The key and path under spec.volumes[1].items[0] exposes the redis-config key from the example-redis-config ConfigMap as \n",
    "   a file named redis.conf on the config volume.\n",
    "3. The config volume is then mounted at /redis-master by spec.containers[0].volumeMounts[1].\n",
    "\n",
    "This has the net effect of exposing the data in data.redis-config from the example-redis-config ConfigMap above as /redis-master/redis.conf inside the Pod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5538680",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8198089d",
   "metadata": {},
   "source": [
    "Examine the created objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29752deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pod/redis configmap/example-redis-config "
   ]
  },
  {
   "cell_type": "raw",
   "id": "36051545",
   "metadata": {},
   "source": [
    "You should see the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2ae5b5b",
   "metadata": {},
   "source": [
    "NAME        READY   STATUS    RESTARTS   AGE\n",
    "pod/redis   1/1     Running   0          8s\n",
    "\n",
    "NAME                             DATA   AGE\n",
    "configmap/example-redis-config   1      14s"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f14947dc",
   "metadata": {},
   "source": [
    "Recall that we left redis-config key in the example-redis-config ConfigMap blank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3960641",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe configmap/example-redis-config"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f287ceb3",
   "metadata": {},
   "source": [
    "You should see an empty redis-config key:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e9a550d",
   "metadata": {},
   "source": [
    "Name:         example-redis-config\n",
    "Namespace:    default\n",
    "Labels:       <none>\n",
    "Annotations:  <none>\n",
    "\n",
    "Data\n",
    "====\n",
    "redis-config:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2208c97",
   "metadata": {},
   "source": [
    "Use kubectl exec to enter the pod and run the redis-cli tool to check the current configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3625e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it redis -- redis-cli"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a8c6075",
   "metadata": {},
   "source": [
    "Check maxmemory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "127.0.0.1:6379> CONFIG GET maxmemory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29ebd409",
   "metadata": {},
   "source": [
    "It should show the default value of 0:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1b32124",
   "metadata": {},
   "source": [
    "1) \"maxmemory\"\n",
    "2) \"0\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c294762",
   "metadata": {},
   "source": [
    "Similarly, check maxmemory-policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "127.0.0.1:6379> CONFIG GET maxmemory-policy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a297c6c",
   "metadata": {},
   "source": [
    "Which should also yield its default value of noeviction:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96c8b915",
   "metadata": {},
   "source": [
    "1) \"maxmemory-policy\"\n",
    "2) \"noeviction\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb3c95d1",
   "metadata": {},
   "source": [
    "Now let's add some configuration values to the example-redis-config ConfigMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d01947",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: example-redis-config\n",
    "data:\n",
    "  redis-config: |\n",
    "    maxmemory 2mb\n",
    "    maxmemory-policy allkeys-lru    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e905aa9",
   "metadata": {},
   "source": [
    "Apply the updated ConfigMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f example-redis-config.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0285032",
   "metadata": {},
   "source": [
    "Confirm that the ConfigMap was updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9126d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe configmap/example-redis-config"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abf2b3bd",
   "metadata": {},
   "source": [
    "You should see the configuration values we just added:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2017ab6e",
   "metadata": {},
   "source": [
    "Name:         example-redis-config\n",
    "Namespace:    default\n",
    "Labels:       <none>\n",
    "Annotations:  <none>\n",
    "\n",
    "Data\n",
    "====\n",
    "redis-config:\n",
    "----\n",
    "maxmemory 2mb\n",
    "maxmemory-policy allkeys-lru"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46750d6a",
   "metadata": {},
   "source": [
    "Check the Redis Pod again using redis-cli via kubectl exec to see if the configuration was applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa85bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it redis -- redis-cli"
   ]
  },
  {
   "cell_type": "raw",
   "id": "018c1d53",
   "metadata": {},
   "source": [
    "Check maxmemory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24259fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "127.0.0.1:6379> CONFIG GET maxmemory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f86b93e7",
   "metadata": {},
   "source": [
    "It remains at the default value of 0:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e12d64c",
   "metadata": {},
   "source": [
    "1) \"maxmemory\"\n",
    "2) \"0\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0fb6516",
   "metadata": {},
   "source": [
    "Similarly, maxmemory-policy remains at the noeviction default setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "127.0.0.1:6379> CONFIG GET maxmemory-policy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da08f88c",
   "metadata": {},
   "source": [
    "Returns:\n",
    "    \n",
    "1) \"maxmemory-policy\"\n",
    "2) \"noeviction\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "746b512c",
   "metadata": {},
   "source": [
    "The configuration values have not changed because the Pod needs to be restarted to grab updated values from associated ConfigMaps. Let's delete and recreate the Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete pod redis\n",
    "kubectl apply -f redis.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c6fe4ac",
   "metadata": {},
   "source": [
    "Now re-check the configuration values one last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f65041",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it redis -- redis-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check maxmemory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "127.0.0.1:6379> CONFIG GET maxmemory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17749fac",
   "metadata": {},
   "source": [
    "It should now return the updated value of 2097152:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43133ce4",
   "metadata": {},
   "source": [
    "1) \"maxmemory\"\n",
    "2) \"2097152\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1d25961",
   "metadata": {},
   "source": [
    "Similarly, maxmemory-policy has also been updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "127.0.0.1:6379> CONFIG GET maxmemory-policy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30ae4ae9",
   "metadata": {},
   "source": [
    "It now reflects the desired value of allkeys-lru:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b017a828",
   "metadata": {},
   "source": [
    "1) \"maxmemory-policy\"\n",
    "2) \"allkeys-lru\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "827f8365",
   "metadata": {},
   "source": [
    "Clean up your work by deleting the created resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5241ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete pod/redis configmap/example-redis-config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48131ab2",
   "metadata": {},
   "source": [
    "# Persistent Volumes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d825ef66",
   "metadata": {},
   "source": [
    "This document describes persistent volumes in Kubernetes. Familiarity with volumes is suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31e385",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7b65d13",
   "metadata": {},
   "source": [
    "Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.\n",
    "\n",
    "A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.\n",
    "\n",
    "A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).\n",
    "\n",
    "While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2819c",
   "metadata": {},
   "source": [
    "# Lifecycle of a volume and claim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff478183",
   "metadata": {},
   "source": [
    "PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7349c3d",
   "metadata": {},
   "source": [
    "### Provisioning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d34d259d",
   "metadata": {},
   "source": [
    "There are two ways PVs may be provisioned: statically or dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb672c42",
   "metadata": {},
   "source": [
    "##### Static"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de9d071c",
   "metadata": {},
   "source": [
    "A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db4ead",
   "metadata": {},
   "source": [
    "#### Dynamic"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fb5e01d",
   "metadata": {},
   "source": [
    "When none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class \"\" effectively disable dynamic provisioning for themselves.\n",
    "\n",
    "To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the --enable-admission-plugins flag of the API server component. For more information on API server command-line flags, check kube-apiserver documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc2185",
   "metadata": {},
   "source": [
    "### Binding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d67be5a",
   "metadata": {},
   "source": [
    "A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.\n",
    "\n",
    "Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9b453",
   "metadata": {},
   "source": [
    "### Using"
   ]
  },
  {
   "cell_type": "raw",
   "id": "268091be",
   "metadata": {},
   "source": [
    "Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.\n",
    "\n",
    "Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a persistentVolumeClaim section in a Pod's volumes block. See Claims As Volumes for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0ccea",
   "metadata": {},
   "source": [
    "### Reclaiming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69119304",
   "metadata": {},
   "source": [
    "When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83779da",
   "metadata": {},
   "source": [
    "##### Retain"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3085001",
   "metadata": {},
   "source": [
    "The Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered \"released\". But it is not yet available for another claim because the previous claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.\n",
    "\n",
    "1. Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure \n",
    "   Disk, or Cinder volume) still exists after the PV is deleted.\n",
    "2. Manually clean up the data on the associated storage asset accordingly.\n",
    "3. Manually delete the associated storage asset.\n",
    "\n",
    "If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05abb0",
   "metadata": {},
   "source": [
    "##### Delete"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b86ed802",
   "metadata": {},
   "source": [
    "For volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the reclaim policy of their StorageClass, which defaults to Delete. The administrator should configure the StorageClass according to users' expectations; otherwise, the PV must be edited or patched after it is created. See Change the Reclaim Policy of a PersistentVolume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568497de",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "raw",
   "id": "671816f4",
   "metadata": {},
   "source": [
    "define a PersistentVolume pv.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: pv-nfs\n",
    "spec:\n",
    "  storageClassName: standard\n",
    "  capacity:\n",
    "    storage: 4Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  persistentVolumeReclaimPolicy: Retain\n",
    "  nfs:\n",
    "    server: 20.243.250.224\n",
    "    path: \"/export/volumes/pod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f pv.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a59ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe pv pv-nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c80194",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e73169bc",
   "metadata": {},
   "source": [
    "define PersistentVolumeClaim pvc.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: pvc-nfs\n",
    "spec:\n",
    "  storageClassName: standard\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b343228b",
   "metadata": {},
   "source": [
    "注意 pvc.yaml 中的 storageClassName 與 pv.yaml 中的 storageClassName 保持一致，否則 k8s 無法把 pvc 連接到 pv 的資源池, 而是會在 /tmp/hostpath-provisioner 目錄下新建一個名字類似於 pvc-[uuid], 如 pvc-d47f4936-dd7c-49b4-a512-6241d3ff15ef, 的文件夾, 以此作為 PVC 的 Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f pvc.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pvc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de36cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe pvc pvc-nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154aba8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f408c6f0",
   "metadata": {},
   "source": [
    "Use persistentvolume in Pod nginx.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ba2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: web\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: web\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: web\n",
    "    spec:\n",
    "      volumes:\n",
    "      - name: webcontent\n",
    "        persistentVolumeClaim:\n",
    "          claimName: pvc-nfs\n",
    "      containers:\n",
    "      - image: nginx\n",
    "        name: nginx\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "        volumeMounts:\n",
    "        - name: webcontent\n",
    "          mountPath: \"/usr/share/nginx/html/web-app\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f nginx.yml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fd704",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc4f0621",
   "metadata": {},
   "source": [
    "在 nfs server 上創建一個文件，具體路徑為:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ba81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vagrant@nfs-server:/export/volumes/pod$ pwd\n",
    "/export/volumes/pod\n",
    "vagrant@nfs-server:/export/volumes/pod$ ls\n",
    "index.html\n",
    "vagrant@nfs-server:/export/volumes/pod$ more index.html\n",
    "hello k8s\n",
    "vagrant@nfs-server:/export/volumes/pod$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74cf6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71483f6b",
   "metadata": {},
   "source": [
    "創建一個 service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f200ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl expose deployment web --port=80 --type=NodePort"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a56409b",
   "metadata": {},
   "source": [
    "查看 service 有沒有創建成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get service"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b226fb84",
   "metadata": {},
   "source": [
    "查看 server 的 url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "minikube service web --url"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d5a2585",
   "metadata": {},
   "source": [
    "後面加上 /web-app/ 應該可以看到 hello k8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db4814",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete service web\n",
    "kubectl delete deployments.apps web\n",
    "kubectl delete persistentvolumeclaims pvc-nfs\n",
    "kubectl delete persistentvolume pv-nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31069e4",
   "metadata": {},
   "source": [
    "### 範例: Configure a Pod to Use a PersistentVolume for Storage"
   ]
  },
  {
   "cell_type": "raw",
   "id": "492153a8",
   "metadata": {},
   "source": [
    "This example shows you how to configure a Pod to use a PersistentVolumeClaim for storage. Here is a summary of the process:\n",
    "\n",
    "1. You, as cluster administrator, create a PersistentVolume backed by physical storage. You do not associate the volume \n",
    "   with any Pod.\n",
    "\n",
    "2. You, now taking the role of a developer / cluster user, create a PersistentVolumeClaim that is automatically bound to a \n",
    "   suitable PersistentVolume.\n",
    "\n",
    "3. You create a Pod that uses the above PersistentVolumeClaim for storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f23af",
   "metadata": {},
   "source": [
    "##### Before you begin"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc938799",
   "metadata": {},
   "source": [
    "1. You need to have a Kubernetes cluster that has only one Node, and the kubectl command-line tool must be configured to \n",
    "   communicate with your cluster. If you do not already have a single-node cluster, you can create one by using Minikube.\n",
    "\n",
    "2. Familiarize yourself with the material in Persistent Volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52160f",
   "metadata": {},
   "source": [
    "##### Create an index.html file on your Node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e11689c2",
   "metadata": {},
   "source": [
    "Open a shell to the single Node in your cluster. How you open a shell depends on how you set up your cluster. For example, if you are using Minikube, you can open a shell to your Node by entering minikube ssh.\n",
    "\n",
    "In your shell on that Node, create a /mnt/data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes that your Node uses \"sudo\" to run commands\n",
    "# as the superuser\n",
    "sudo mkdir /mnt/data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57c35ee8",
   "metadata": {},
   "source": [
    "In the /mnt/data directory, create an index.html file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This again assumes that your Node uses \"sudo\" to run commands\n",
    "# as the superuser\n",
    "sudo sh -c \"echo 'Hello from Kubernetes storage' > /mnt/data/index.html\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "045c0e51",
   "metadata": {},
   "source": [
    "Test that the index.html file exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6284a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /mnt/data/index.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "835fbf5d",
   "metadata": {},
   "source": [
    "The output should be:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dda0cc8e",
   "metadata": {},
   "source": [
    "Helle"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d89d8f28",
   "metadata": {},
   "source": [
    "You can now close the shell to your Node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951c77c",
   "metadata": {},
   "source": [
    "##### Create a PersistentVolume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97509e7c",
   "metadata": {},
   "source": [
    "In this exercise, you create a hostPath PersistentVolume. Kubernetes supports hostPath for development and testing on a single-node cluster. A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.\n",
    "\n",
    "In a production cluster, you would not use hostPath. Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also use StorageClasses to set up dynamic provisioning.\n",
    "\n",
    "Here is the configuration file for the hostPath PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: task-pv-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 10Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/mnt/data\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "af616cb2",
   "metadata": {},
   "source": [
    "The configuration file specifies that the volume is at /mnt/data on the cluster's Node. The configuration also specifies a size of 10 gibibytes and an access mode of ReadWriteOnce, which means the volume can be mounted as read-write by a single Node. It defines the StorageClass name manual for the PersistentVolume, which will be used to bind PersistentVolumeClaim requests to this PersistentVolume.\n",
    "\n",
    "Create the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/storage/pv-volume.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "039aad94",
   "metadata": {},
   "source": [
    "View information about the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pv task-pv-volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78102f70",
   "metadata": {},
   "source": [
    "The output shows that the PersistentVolume has a STATUS of Available. This means it has not yet been bound to a PersistentVolumeClaim."
   ]
  },
  {
   "cell_type": "raw",
   "id": "70666044",
   "metadata": {},
   "source": [
    "NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE\n",
    "task-pv-volume   10Gi       RWO           Retain          Available             manual                   4s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae95b58",
   "metadata": {},
   "source": [
    "##### Create a PersistentVolumeClaim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea1dd7c2",
   "metadata": {},
   "source": [
    "The next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims to request physical storage. In this exercise, you create a PersistentVolumeClaim that requests a volume of at least three gibibytes that can provide read-write access for at least one Node.\n",
    "\n",
    "Here is the configuration file for the PersistentVolumeClaim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: task-pv-claim\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 3Gi"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7d22818",
   "metadata": {},
   "source": [
    "Create the PersistentVolumeClaim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/storage/pv-claim.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa5061a1",
   "metadata": {},
   "source": [
    "After you create the PersistentVolumeClaim, the Kubernetes control plane looks for a PersistentVolume that satisfies the claim's requirements. If the control plane finds a suitable PersistentVolume with the same StorageClass, it binds the claim to the volume.\n",
    "\n",
    "Look again at the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e11b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pv task-pv-volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4b3cdaa",
   "metadata": {},
   "source": [
    "Now the output shows a STATUS of Bound.\n",
    "\n",
    "NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                   STORAGECLASS   REASON    AGE\n",
    "task-pv-volume   10Gi       RWO           Retain          Bound     default/task-pv-claim   manual                   2m"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fedd69d",
   "metadata": {},
   "source": [
    "Look at the PersistentVolumeClaim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pvc task-pv-claim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae9d76a2",
   "metadata": {},
   "source": [
    "The output shows that the PersistentVolumeClaim is bound to your PersistentVolume, task-pv-volume.\n",
    "\n",
    "NAME            STATUS    VOLUME           CAPACITY   ACCESSMODES   STORAGECLASS   AGE\n",
    "task-pv-claim   Bound     task-pv-volume   10Gi       RWO           manual         30s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c285b4",
   "metadata": {},
   "source": [
    "##### Create a Pod"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca04d6da",
   "metadata": {},
   "source": [
    "The next step is to create a Pod that uses your PersistentVolumeClaim as a volume.\n",
    "Here is the configuration file for the Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: task-pv-pod\n",
    "spec:\n",
    "  volumes:\n",
    "    - name: task-pv-storage\n",
    "      persistentVolumeClaim:\n",
    "        claimName: task-pv-claim\n",
    "  containers:\n",
    "    - name: task-pv-container\n",
    "      image: nginx\n",
    "      ports:\n",
    "        - containerPort: 80\n",
    "          name: \"http-server\"\n",
    "      volumeMounts:\n",
    "        - mountPath: \"/usr/share/nginx/html\"\n",
    "          name: task-pv-storage"
   ]
  },
  {
   "cell_type": "raw",
   "id": "974f0e33",
   "metadata": {},
   "source": [
    "Notice that the Pod's configuration file specifies a PersistentVolumeClaim, but it does not specify a PersistentVolume. From the Pod's point of view, the claim is a volume.\n",
    "\n",
    "Create the Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/storage/pv-pod.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5456817b",
   "metadata": {},
   "source": [
    "Verify that the container in the Pod is running;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ffd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pod task-pv-pod"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e3d01b8",
   "metadata": {},
   "source": [
    "Get a shell to the container running in your Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64468bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it task-pv-pod -- /bin/bash"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bd37a14",
   "metadata": {},
   "source": [
    "In your shell, verify that nginx is serving the index.html file from the hostPath volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de78861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to run these 3 commands inside the root shell that comes from\n",
    "# running \"kubectl exec\" in the previous step\n",
    "apt update\n",
    "apt install curl\n",
    "curl http://localhost/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4986bf1",
   "metadata": {},
   "source": [
    "The output shows the text that you wrote to the index.html file on the hostPath volume:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c8f33f4",
   "metadata": {},
   "source": [
    "Hello from Kubernetes storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "If you see that message, you have successfully configured a Pod to use storage from a PersistentVolumeClaim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f2b9f",
   "metadata": {},
   "source": [
    "##### Clean up"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b86a574",
   "metadata": {},
   "source": [
    "Delete the Pod, the PersistentVolumeClaim and the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa84af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete pod task-pv-pod\n",
    "kubectl delete pvc task-pv-claim\n",
    "kubectl delete pv task-pv-volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b527157",
   "metadata": {},
   "source": [
    "If you don't already have a shell open to the Node in your cluster, open a new shell the same way that you did earlier.\n",
    "\n",
    "In the shell on your Node, remove the file and directory that you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes that your Node uses \"sudo\" to run commands\n",
    "# as the superuser\n",
    "sudo rm /mnt/data/index.html\n",
    "sudo rmdir /mnt/data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "991ff804",
   "metadata": {},
   "source": [
    "You can now close the shell to your Node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21979dbd",
   "metadata": {},
   "source": [
    "##### Mounting the same persistentVolume in two places"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9d34a56",
   "metadata": {},
   "source": [
    "注意要在 nfs 指定的資料夾中先加入 nginx.conf 檔，如果不加入，同步之後，因為 nfs 沒有 nginx.conf 檔，所以 nginx 的 nginx.conf 檔也會被刪除了。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c179e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test\n",
    "spec:\n",
    "  containers:\n",
    "    - name: test\n",
    "      image: nginx\n",
    "      volumeMounts:\n",
    "        # a mount for site-data\n",
    "        - name: config\n",
    "          mountPath: /usr/share/nginx/html\n",
    "          subPath: html\n",
    "        # another mount for nginx config\n",
    "        - name: config\n",
    "          mountPath: /etc/nginx/nginx.conf\n",
    "          subPath: nginx.conf\n",
    "  volumes:\n",
    "    - name: config\n",
    "      persistentVolumeClaim:\n",
    "        claimName: task-nfs-claim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3013993b",
   "metadata": {},
   "source": [
    "You can perform 2 volume mounts on your nginx container:\n",
    "\n",
    "/usr/share/nginx/html for the static website /etc/nginx/nginx.conf for the default config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427e786",
   "metadata": {},
   "source": [
    "### Access control"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0c5de67",
   "metadata": {},
   "source": [
    "Storage configured with a group ID (GID) allows writing only by Pods using the same GID. Mismatched or missing GIDs cause permission denied errors. To reduce the need for coordination with users, an administrator can annotate a PersistentVolume with a GID. Then the GID is automatically added to any Pod that uses the PersistentVolume.\n",
    "\n",
    "Use the pv.beta.kubernetes.io/gid annotation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f110e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: pv1\n",
    "  annotations:\n",
    "    pv.beta.kubernetes.io/gid: \"1234\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e5bd419",
   "metadata": {},
   "source": [
    "When a Pod consumes a PersistentVolume that has a GID annotation, the annotated GID is applied to all containers in the Pod in the same way that GIDs specified in the Pod's security context are. Every GID, whether it originates from a PersistentVolume annotation or the Pod's specification, is applied to the first process run in each container."
   ]
  },
  {
   "cell_type": "raw",
   "id": "020c310d",
   "metadata": {},
   "source": [
    "Note: When a Pod consumes a PersistentVolume, the GIDs associated with the PersistentVolume are not present on the Pod resource itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad2c9f",
   "metadata": {},
   "source": [
    "### StorageClass "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c9b770c",
   "metadata": {},
   "source": [
    "待補"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f24de46",
   "metadata": {},
   "source": [
    "### Dynamic Volume Provisioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd3895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42f887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5b080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216ab25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
