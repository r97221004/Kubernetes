{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de5a43f",
   "metadata": {},
   "source": [
    "# Volumes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdc1e515",
   "metadata": {},
   "source": [
    "On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. One problem is the loss of files when a container crashes. The kubelet restarts the container but with a clean state. A second problem occurs when sharing files between containers running together in a Pod. The Kubernetes volume abstraction solves both of these problems. Familiarity with Pods is suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a9a39",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcb5d32f",
   "metadata": {},
   "source": [
    "Docker has a concept of volumes, though it is somewhat looser and less managed. A Docker volume is a directory on disk or in another container. Docker provides volume drivers, but the functionality is somewhat limited.\n",
    "\n",
    "Kubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously. Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond the lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes; however, Kubernetes does not destroy persistent volumes. For any kind of volume in a given pod, data is preserved across container restarts.\n",
    "\n",
    "At its core, a volume is a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.\n",
    "\n",
    "To use a volume, specify the volumes to provide for the Pod in .spec.volumes and declare where to mount those volumes into containers in .spec.containers[*].volumeMounts. A process in a container sees a filesystem view composed from the initial contents of the container image, plus volumes (if defined) mounted inside the container. The process sees a root filesystem that initially matches the contents of the container image. Any writes to within that filesystem hierarchy, if allowed, affect what that process views when it performs a subsequent filesystem access. Volumes mount at the specified paths within the image. For each container defined within a Pod, you must independently specify where to mount each volume that the container uses.\n",
    "\n",
    "Volumes cannot mount within other volumes (but see Using subPath for a related mechanism). Also, a volume cannot contain a hard link to anything in a different volume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ad9625",
   "metadata": {},
   "source": [
    "# Types of volumes "
   ]
  },
  {
   "cell_type": "raw",
   "id": "14177068",
   "metadata": {},
   "source": [
    "Kubernetes supports several types of volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1c06a",
   "metadata": {},
   "source": [
    "### emptyDir"
   ]
  },
  {
   "cell_type": "raw",
   "id": "185b2082",
   "metadata": {},
   "source": [
    "An emptyDir volume is first created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. As the name says, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently."
   ]
  },
  {
   "cell_type": "raw",
   "id": "376eed47",
   "metadata": {},
   "source": [
    "Note: A container crashing does not remove a Pod from a node. The data in an emptyDir volume is safe across container crashes."
   ]
  },
  {
   "cell_type": "raw",
   "id": "73f78f8a",
   "metadata": {},
   "source": [
    "Some uses for an emptyDir are:\n",
    "\n",
    "scratch space, such as for a disk-based merge sort\n",
    "checkpointing a long computation for recovery from crashes\n",
    "holding files that a content-manager container fetches while a webserver container serves the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ca2d152",
   "metadata": {},
   "source": [
    "Depending on your environment, emptyDir volumes are stored on whatever medium that backs the node such as disk or SSD, or network storage. However, if you set the emptyDir.medium field to \"Memory\", Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on node reboot and any files you write count against your container's memory limit."
   ]
  },
  {
   "cell_type": "raw",
   "id": "43c9ad73",
   "metadata": {},
   "source": [
    "Note: If the SizeMemoryBackedVolumes feature gate is enabled, you can specify a size for memory backed volumes. If no size is specified, memory backed volumes are sized to 50% of the memory on a Linux host."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bde82507",
   "metadata": {},
   "source": [
    "emptyDir configuration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-pd\n",
    "spec:\n",
    "  containers:\n",
    "  - image: registry.k8s.io/test-webserver\n",
    "    name: test-container\n",
    "    volumeMounts:\n",
    "    - mountPath: /cache\n",
    "      name: cache-volume\n",
    "  volumes:\n",
    "  - name: cache-volume\n",
    "    emptyDir: {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334fe29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89e6071e",
   "metadata": {},
   "source": [
    "每當我們建立一個新的 Pod 物件時，Kubernetes 就會在這個 Pod 裏建立一個 emptyDir，該 Pod 中所有的 container 都可以讀寫 emptyDir 中的資料。當 Pod 從 Node 中被移除時，emptyDir 也會隨之消失，emptyDir 有以下幾個用途："
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a831a72",
   "metadata": {},
   "source": [
    "1. 暫時性儲存空間\n",
    "   例如某些應用程式運行時需要一些臨時而無需永久保存的資料夾\n",
    "2. 共用儲存空間\n",
    "   正如上述提到，同一個 Pod 中所有的 containers 都可以讀寫 emptyDir，也可以將 emptyDir 當作是這些 containers 的共用目錄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651dcc2",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818281d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: multicontainer-pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: producer\n",
    "    image: busybox\n",
    "    command: [\"sh\", \"-c\", \"while true; do echo $(hostname) $(date) >> /var/log/index.html; sleep 10; done\"]\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /var/log\n",
    "  - name: consumer\n",
    "    image: nginx\n",
    "    ports:\n",
    "      - containerPort: 80\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /usr/share/nginx/html\n",
    "  volumes:\n",
    "  - name: webcontent\n",
    "    emptyDir: {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1895e26",
   "metadata": {},
   "source": [
    "### hostPath"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48d4a551",
   "metadata": {},
   "source": [
    "Warning:\n",
    "HostPath volumes present many security risks, and it is a best practice to avoid the use of HostPaths when possible. When a HostPath volume must be used, it should be scoped to only the required file or directory, and mounted as ReadOnly.\n",
    "\n",
    "If restricting HostPath access to specific directories through AdmissionPolicy, volumeMounts MUST be required to use readOnly mounts for the policy to be effective."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f8ceaee",
   "metadata": {},
   "source": [
    "A hostPath volume mounts a file or directory from the host node's filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch(緊急出口) for some applications.\n",
    "\n",
    "For example, some uses for a hostPath are:\n",
    "\n",
    "running a container that needs access to Docker internals; use a hostPath of /var/lib/docker\n",
    "running cAdvisor in a container; use a hostPath of /sys\n",
    "allowing a Pod to specify whether a given hostPath should exist prior to the Pod running, whether it should be created, and what it should exist as"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56366590",
   "metadata": {},
   "source": [
    "In addition to the required path property, you can optionally specify a type for a hostPath volume."
   ]
  },
  {
   "cell_type": "raw",
   "id": "029d6ac0",
   "metadata": {},
   "source": [
    "The supported values for field type are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f38f76",
   "metadata": {},
   "source": [
    "<img src='./img/30.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63c7da5c",
   "metadata": {},
   "source": [
    "Watch out when using this type of volume, because:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a7f46d4",
   "metadata": {},
   "source": [
    "1. HostPaths can expose privileged system credentials (such as for the Kubelet) or privileged APIs (such as container r\n",
    "   untime socket), which can be used for container escape or to attack other parts of the cluster.\n",
    "2. Pods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to \n",
    "   different files on the nodes\n",
    "3. The files or directories created on the underlying hosts are only writable by root. You either need to run your process \n",
    "   as root in a privileged Container or modify the file permissions on the host to be able to write to a hostPath volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc5da52c",
   "metadata": {},
   "source": [
    "hostPath configuration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e55ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-pd\n",
    "spec:\n",
    "  containers:\n",
    "  - image: registry.k8s.io/test-webserver\n",
    "    name: test-container\n",
    "    volumeMounts:\n",
    "    - mountPath: /test-pd\n",
    "      name: test-volume\n",
    "  volumes:\n",
    "  - name: test-volume\n",
    "    hostPath:\n",
    "      # directory location on host\n",
    "      path: /data\n",
    "      # this field is optional\n",
    "      type: Directory"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2828e8a",
   "metadata": {},
   "source": [
    "Caution: The FileOrCreate mode does not create the parent directory of the file. If the parent directory of the mounted file does not exist, the pod fails to start. To ensure that this mode works, you can try to mount directories and files separately, as shown in the FileOrCreateconfiguration."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2e70be9",
   "metadata": {},
   "source": [
    "hostPath FileOrCreate configuration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-webserver\n",
    "spec:\n",
    "  containers:\n",
    "  - name: test-webserver\n",
    "    image: registry.k8s.io/test-webserver:latest\n",
    "    volumeMounts:\n",
    "    - mountPath: /var/local/aaa\n",
    "      name: mydir\n",
    "    - mountPath: /var/local/aaa/1.txt\n",
    "      name: myfile\n",
    "  volumes:\n",
    "  - name: mydir\n",
    "    hostPath:\n",
    "      # Ensure the file directory is created.\n",
    "      path: /var/local/aaa\n",
    "      type: DirectoryOrCreate\n",
    "  - name: myfile\n",
    "    hostPath:\n",
    "      path: /var/local/aaa/1.txt\n",
    "      type: FileOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3c6ed",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f5fb032",
   "metadata": {},
   "source": [
    "注意我們的 minikube 是容器，所以要進去 minikube 容器裡面看 /tmp 是否有同步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: multicontainer-pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: producer\n",
    "    image: busybox\n",
    "    command: [\"sh\", \"-c\", \"while true; do echo $(hostname) $(date) >> /var/log/index.html; sleep 10; done\"]\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /var/log\n",
    "  - name: consumer\n",
    "    image: nginx\n",
    "    ports:\n",
    "      - containerPort: 80\n",
    "    volumeMounts:\n",
    "    - name: webcontent\n",
    "      mountPath: /usr/share/nginx/html\n",
    "  volumes:\n",
    "  - name: webcontent\n",
    "    hostPath:\n",
    "      path: /tmp\n",
    "      type: Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba89d7a",
   "metadata": {},
   "source": [
    "### local"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d73c3846",
   "metadata": {},
   "source": [
    "A local volume represents a mounted local storage device such as a disk, partition or directory.\n",
    "\n",
    "Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.\n",
    "\n",
    "Compared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to nodes. The system is aware of the volume's node constraints by looking at the node affinity on the PersistentVolume.\n",
    "\n",
    "However, local volumes are subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.\n",
    "\n",
    "The following example shows a PersistentVolume using a local volume and nodeAffinity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80922b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: example-pv\n",
    "spec:\n",
    "  capacity:\n",
    "    storage: 100Gi\n",
    "  volumeMode: Filesystem\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  persistentVolumeReclaimPolicy: Delete\n",
    "  storageClassName: local-storage\n",
    "  local:\n",
    "    path: /mnt/disks/ssd1\n",
    "  nodeAffinity:\n",
    "    required:\n",
    "      nodeSelectorTerms:\n",
    "      - matchExpressions:\n",
    "        - key: kubernetes.io/hostname\n",
    "          operator: In\n",
    "          values:\n",
    "          - example-node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46a23ec8",
   "metadata": {},
   "source": [
    "You must set a PersistentVolume nodeAffinity when using local volumes. The Kubernetes scheduler uses the PersistentVolume nodeAffinity to schedule these Pods to the correct node.\n",
    "\n",
    "PersistentVolume volumeMode can be set to \"Block\" (instead of the default value \"Filesystem\") to expose the local volume as a raw block device.\n",
    "\n",
    "When using local volumes, it is recommended to create a StorageClass with volumeBindingMode set to WaitForFirstConsumer. For more details, see the local StorageClass example. Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have, such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.\n",
    "\n",
    "An external static provisioner can be run separately for improved management of the local volume lifecycle. Note that this provisioner does not support dynamic provisioning yet. For an example on how to run an external local provisioner, see the local volume provisioner user guide."
   ]
  },
  {
   "cell_type": "raw",
   "id": "40c44276",
   "metadata": {},
   "source": [
    "Note: The local PersistentVolume requires manual cleanup and deletion by the user if the external static provisioner is not used to manage the volume lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "待補"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f16c1",
   "metadata": {},
   "source": [
    "### nfs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "361516c4",
   "metadata": {},
   "source": [
    "An nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be shared between pods. NFS can be mounted by multiple writers simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test-pd\n",
    "spec:\n",
    "  containers:\n",
    "  - image: registry.k8s.io/test-webserver\n",
    "    name: test-container\n",
    "    volumeMounts:\n",
    "    - mountPath: /my-nfs-data\n",
    "      name: test-volume\n",
    "  volumes:\n",
    "  - name: test-volume\n",
    "    nfs:\n",
    "      server: my-nfs-server.example.com\n",
    "      path: /my-nfs-volume\n",
    "      readOnly: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48131ab2",
   "metadata": {},
   "source": [
    "# Persistent Volumes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d825ef66",
   "metadata": {},
   "source": [
    "This document describes persistent volumes in Kubernetes. Familiarity with volumes is suggested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31e385",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7b65d13",
   "metadata": {},
   "source": [
    "Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.\n",
    "\n",
    "A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.\n",
    "\n",
    "A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes).\n",
    "\n",
    "While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2819c",
   "metadata": {},
   "source": [
    "# Lifecycle of a volume and claim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff478183",
   "metadata": {},
   "source": [
    "PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7349c3d",
   "metadata": {},
   "source": [
    "### Provisioning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d34d259d",
   "metadata": {},
   "source": [
    "There are two ways PVs may be provisioned: statically or dynamically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb672c42",
   "metadata": {},
   "source": [
    "##### Static"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de9d071c",
   "metadata": {},
   "source": [
    "A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db4ead",
   "metadata": {},
   "source": [
    "#### Dynamic"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fb5e01d",
   "metadata": {},
   "source": [
    "When none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class \"\" effectively disable dynamic provisioning for themselves.\n",
    "\n",
    "To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the --enable-admission-plugins flag of the API server component. For more information on API server command-line flags, check kube-apiserver documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc2185",
   "metadata": {},
   "source": [
    "### Binding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d67be5a",
   "metadata": {},
   "source": [
    "A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.\n",
    "\n",
    "Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9b453",
   "metadata": {},
   "source": [
    "### Using"
   ]
  },
  {
   "cell_type": "raw",
   "id": "268091be",
   "metadata": {},
   "source": [
    "Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.\n",
    "\n",
    "Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a persistentVolumeClaim section in a Pod's volumes block. See Claims As Volumes for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0ccea",
   "metadata": {},
   "source": [
    "### Reclaiming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69119304",
   "metadata": {},
   "source": [
    "When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83779da",
   "metadata": {},
   "source": [
    "##### Retain"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3085001",
   "metadata": {},
   "source": [
    "The Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered \"released\". But it is not yet available for another claim because the previous claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.\n",
    "\n",
    "1. Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure \n",
    "   Disk, or Cinder volume) still exists after the PV is deleted.\n",
    "2. Manually clean up the data on the associated storage asset accordingly.\n",
    "3. Manually delete the associated storage asset.\n",
    "\n",
    "If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05abb0",
   "metadata": {},
   "source": [
    "##### Delete"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b86ed802",
   "metadata": {},
   "source": [
    "For volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the reclaim policy of their StorageClass, which defaults to Delete. The administrator should configure the StorageClass according to users' expectations; otherwise, the PV must be edited or patched after it is created. See Change the Reclaim Policy of a PersistentVolume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568497de",
   "metadata": {},
   "source": [
    "### 範例"
   ]
  },
  {
   "cell_type": "raw",
   "id": "671816f4",
   "metadata": {},
   "source": [
    "define a PersistentVolume pv.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: pv-nfs\n",
    "spec:\n",
    "  storageClassName: standard\n",
    "  capacity:\n",
    "    storage: 4Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  persistentVolumeReclaimPolicy: Retain\n",
    "  nfs:\n",
    "    server: 20.243.250.224\n",
    "    path: \"/export/volumes/pod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f pv.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a59ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe pv pv-nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c80194",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e73169bc",
   "metadata": {},
   "source": [
    "define PersistentVolumeClaim pvc.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: pvc-nfs\n",
    "spec:\n",
    "  storageClassName: standard\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b343228b",
   "metadata": {},
   "source": [
    "注意 pvc.yaml 中的 storageClassName 與 pv.yaml 中的 storageClassName 保持一致，否則 k8s 無法把 pvc 連接到 pv 的資源池, 而是會在 /tmp/hostpath-provisioner 目錄下新建一個名字類似於 pvc-[uuid], 如 pvc-d47f4936-dd7c-49b4-a512-6241d3ff15ef, 的文件夾, 以此作為 PVC 的 Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f pvc.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pvc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de36cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe pvc pvc-nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154aba8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f408c6f0",
   "metadata": {},
   "source": [
    "Use persistentvolume in Pod nginx.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ba2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: web\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: web\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: web\n",
    "    spec:\n",
    "      volumes:\n",
    "      - name: webcontent\n",
    "        persistentVolumeClaim:\n",
    "          claimName: pvc-nfs\n",
    "      containers:\n",
    "      - image: nginx\n",
    "        name: nginx\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "        volumeMounts:\n",
    "        - name: webcontent\n",
    "          mountPath: \"/usr/share/nginx/html/web-app\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f nginx.yml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fd704",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc4f0621",
   "metadata": {},
   "source": [
    "在 nfs server 上創建一個文件，具體路徑為:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ba81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vagrant@nfs-server:/export/volumes/pod$ pwd\n",
    "/export/volumes/pod\n",
    "vagrant@nfs-server:/export/volumes/pod$ ls\n",
    "index.html\n",
    "vagrant@nfs-server:/export/volumes/pod$ more index.html\n",
    "hello k8s\n",
    "vagrant@nfs-server:/export/volumes/pod$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74cf6e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71483f6b",
   "metadata": {},
   "source": [
    "創建一個 service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f200ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl expose deployment web --port=80 --type=NodePort"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a56409b",
   "metadata": {},
   "source": [
    "查看 service 有沒有創建成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get service"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b226fb84",
   "metadata": {},
   "source": [
    "查看 server 的 url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "minikube service web --url"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d5a2585",
   "metadata": {},
   "source": [
    "後面加上 /web-app/ 應該可以看到 hello k8s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db4814",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete service web\n",
    "kubectl delete deployments.apps web\n",
    "kubectl delete persistentvolumeclaims pvc-nfs\n",
    "kubectl delete persistentvolume pv-nfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31069e4",
   "metadata": {},
   "source": [
    "### 範例: Configure a Pod to Use a PersistentVolume for Storage"
   ]
  },
  {
   "cell_type": "raw",
   "id": "492153a8",
   "metadata": {},
   "source": [
    "This example shows you how to configure a Pod to use a PersistentVolumeClaim for storage. Here is a summary of the process:\n",
    "\n",
    "1. You, as cluster administrator, create a PersistentVolume backed by physical storage. You do not associate the volume \n",
    "   with any Pod.\n",
    "\n",
    "2. You, now taking the role of a developer / cluster user, create a PersistentVolumeClaim that is automatically bound to a \n",
    "   suitable PersistentVolume.\n",
    "\n",
    "3. You create a Pod that uses the above PersistentVolumeClaim for storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f23af",
   "metadata": {},
   "source": [
    "##### Before you begin"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc938799",
   "metadata": {},
   "source": [
    "1. You need to have a Kubernetes cluster that has only one Node, and the kubectl command-line tool must be configured to \n",
    "   communicate with your cluster. If you do not already have a single-node cluster, you can create one by using Minikube.\n",
    "\n",
    "2. Familiarize yourself with the material in Persistent Volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52160f",
   "metadata": {},
   "source": [
    "##### Create an index.html file on your Node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e11689c2",
   "metadata": {},
   "source": [
    "Open a shell to the single Node in your cluster. How you open a shell depends on how you set up your cluster. For example, if you are using Minikube, you can open a shell to your Node by entering minikube ssh.\n",
    "\n",
    "In your shell on that Node, create a /mnt/data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes that your Node uses \"sudo\" to run commands\n",
    "# as the superuser\n",
    "sudo mkdir /mnt/data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57c35ee8",
   "metadata": {},
   "source": [
    "In the /mnt/data directory, create an index.html file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This again assumes that your Node uses \"sudo\" to run commands\n",
    "# as the superuser\n",
    "sudo sh -c \"echo 'Hello from Kubernetes storage' > /mnt/data/index.html\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "045c0e51",
   "metadata": {},
   "source": [
    "Test that the index.html file exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6284a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat /mnt/data/index.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "835fbf5d",
   "metadata": {},
   "source": [
    "The output should be:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dda0cc8e",
   "metadata": {},
   "source": [
    "Helle"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d89d8f28",
   "metadata": {},
   "source": [
    "You can now close the shell to your Node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951c77c",
   "metadata": {},
   "source": [
    "##### Create a PersistentVolume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97509e7c",
   "metadata": {},
   "source": [
    "In this exercise, you create a hostPath PersistentVolume. Kubernetes supports hostPath for development and testing on a single-node cluster. A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.\n",
    "\n",
    "In a production cluster, you would not use hostPath. Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also use StorageClasses to set up dynamic provisioning.\n",
    "\n",
    "Here is the configuration file for the hostPath PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: task-pv-volume\n",
    "  labels:\n",
    "    type: local\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  capacity:\n",
    "    storage: 10Gi\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  hostPath:\n",
    "    path: \"/mnt/data\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "af616cb2",
   "metadata": {},
   "source": [
    "The configuration file specifies that the volume is at /mnt/data on the cluster's Node. The configuration also specifies a size of 10 gibibytes and an access mode of ReadWriteOnce, which means the volume can be mounted as read-write by a single Node. It defines the StorageClass name manual for the PersistentVolume, which will be used to bind PersistentVolumeClaim requests to this PersistentVolume.\n",
    "\n",
    "Create the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/storage/pv-volume.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "039aad94",
   "metadata": {},
   "source": [
    "View information about the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pv task-pv-volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "78102f70",
   "metadata": {},
   "source": [
    "The output shows that the PersistentVolume has a STATUS of Available. This means it has not yet been bound to a PersistentVolumeClaim."
   ]
  },
  {
   "cell_type": "raw",
   "id": "70666044",
   "metadata": {},
   "source": [
    "NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE\n",
    "task-pv-volume   10Gi       RWO           Retain          Available             manual                   4s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae95b58",
   "metadata": {},
   "source": [
    "##### Create a PersistentVolumeClaim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea1dd7c2",
   "metadata": {},
   "source": [
    "The next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims to request physical storage. In this exercise, you create a PersistentVolumeClaim that requests a volume of at least three gibibytes that can provide read-write access for at least one Node.\n",
    "\n",
    "Here is the configuration file for the PersistentVolumeClaim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: task-pv-claim\n",
    "spec:\n",
    "  storageClassName: manual\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 3Gi"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e7d22818",
   "metadata": {},
   "source": [
    "Create the PersistentVolumeClaim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/storage/pv-claim.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fa5061a1",
   "metadata": {},
   "source": [
    "After you create the PersistentVolumeClaim, the Kubernetes control plane looks for a PersistentVolume that satisfies the claim's requirements. If the control plane finds a suitable PersistentVolume with the same StorageClass, it binds the claim to the volume.\n",
    "\n",
    "Look again at the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e11b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pv task-pv-volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4b3cdaa",
   "metadata": {},
   "source": [
    "Now the output shows a STATUS of Bound.\n",
    "\n",
    "NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                   STORAGECLASS   REASON    AGE\n",
    "task-pv-volume   10Gi       RWO           Retain          Bound     default/task-pv-claim   manual                   2m"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8fedd69d",
   "metadata": {},
   "source": [
    "Look at the PersistentVolumeClaim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pvc task-pv-claim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae9d76a2",
   "metadata": {},
   "source": [
    "The output shows that the PersistentVolumeClaim is bound to your PersistentVolume, task-pv-volume.\n",
    "\n",
    "NAME            STATUS    VOLUME           CAPACITY   ACCESSMODES   STORAGECLASS   AGE\n",
    "task-pv-claim   Bound     task-pv-volume   10Gi       RWO           manual         30s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c285b4",
   "metadata": {},
   "source": [
    "##### Create a Pod"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca04d6da",
   "metadata": {},
   "source": [
    "The next step is to create a Pod that uses your PersistentVolumeClaim as a volume.\n",
    "Here is the configuration file for the Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: task-pv-pod\n",
    "spec:\n",
    "  volumes:\n",
    "    - name: task-pv-storage\n",
    "      persistentVolumeClaim:\n",
    "        claimName: task-pv-claim\n",
    "  containers:\n",
    "    - name: task-pv-container\n",
    "      image: nginx\n",
    "      ports:\n",
    "        - containerPort: 80\n",
    "          name: \"http-server\"\n",
    "      volumeMounts:\n",
    "        - mountPath: \"/usr/share/nginx/html\"\n",
    "          name: task-pv-storage"
   ]
  },
  {
   "cell_type": "raw",
   "id": "974f0e33",
   "metadata": {},
   "source": [
    "Notice that the Pod's configuration file specifies a PersistentVolumeClaim, but it does not specify a PersistentVolume. From the Pod's point of view, the claim is a volume.\n",
    "\n",
    "Create the Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f https://k8s.io/examples/pods/storage/pv-pod.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5456817b",
   "metadata": {},
   "source": [
    "Verify that the container in the Pod is running;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ffd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pod task-pv-pod"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e3d01b8",
   "metadata": {},
   "source": [
    "Get a shell to the container running in your Pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64468bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it task-pv-pod -- /bin/bash"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bd37a14",
   "metadata": {},
   "source": [
    "In your shell, verify that nginx is serving the index.html file from the hostPath volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de78861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to run these 3 commands inside the root shell that comes from\n",
    "# running \"kubectl exec\" in the previous step\n",
    "apt update\n",
    "apt install curl\n",
    "curl http://localhost/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4986bf1",
   "metadata": {},
   "source": [
    "The output shows the text that you wrote to the index.html file on the hostPath volume:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c8f33f4",
   "metadata": {},
   "source": [
    "Hello from Kubernetes storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "If you see that message, you have successfully configured a Pod to use storage from a PersistentVolumeClaim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f2b9f",
   "metadata": {},
   "source": [
    "##### Clean up"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b86a574",
   "metadata": {},
   "source": [
    "Delete the Pod, the PersistentVolumeClaim and the PersistentVolume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa84af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete pod task-pv-pod\n",
    "kubectl delete pvc task-pv-claim\n",
    "kubectl delete pv task-pv-volume"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b527157",
   "metadata": {},
   "source": [
    "If you don't already have a shell open to the Node in your cluster, open a new shell the same way that you did earlier.\n",
    "\n",
    "In the shell on your Node, remove the file and directory that you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes that your Node uses \"sudo\" to run commands\n",
    "# as the superuser\n",
    "sudo rm /mnt/data/index.html\n",
    "sudo rmdir /mnt/data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "991ff804",
   "metadata": {},
   "source": [
    "You can now close the shell to your Node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21979dbd",
   "metadata": {},
   "source": [
    "##### Mounting the same persistentVolume in two places"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9b8b7fe",
   "metadata": {},
   "source": [
    "注意要在 nfs 指定的資料夾中先加入 nginx.conf 檔，如果不加入，同步之後，因為 nfs 沒有 nginx.conf 檔，所以 nginx 的 nginx.conf 檔也會被刪除了。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c179e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: test\n",
    "spec:\n",
    "  containers:\n",
    "    - name: test\n",
    "      image: nginx\n",
    "      volumeMounts:\n",
    "        # a mount for site-data\n",
    "        - name: config\n",
    "          mountPath: /usr/share/nginx/html\n",
    "          subPath: html\n",
    "        # another mount for nginx config\n",
    "        - name: config\n",
    "          mountPath: /etc/nginx/nginx.conf\n",
    "          subPath: nginx.conf\n",
    "  volumes:\n",
    "    - name: config\n",
    "      persistentVolumeClaim:\n",
    "        claimName: task-nfs-claim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd5f1c0e",
   "metadata": {},
   "source": [
    "You can perform 2 volume mounts on your nginx container:\n",
    "\n",
    "/usr/share/nginx/html for the static website /etc/nginx/nginx.conf for the default config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab95405",
   "metadata": {},
   "source": [
    "### Access control"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fab44753",
   "metadata": {},
   "source": [
    "Storage configured with a group ID (GID) allows writing only by Pods using the same GID. Mismatched or missing GIDs cause permission denied errors. To reduce the need for coordination with users, an administrator can annotate a PersistentVolume with a GID. Then the GID is automatically added to any Pod that uses the PersistentVolume.\n",
    "\n",
    "Use the pv.beta.kubernetes.io/gid annotation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f44169",
   "metadata": {},
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: pv1\n",
    "  annotations:\n",
    "    pv.beta.kubernetes.io/gid: \"1234\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ccf07bb",
   "metadata": {},
   "source": [
    "When a Pod consumes a PersistentVolume that has a GID annotation, the annotated GID is applied to all containers in the Pod in the same way that GIDs specified in the Pod's security context are. Every GID, whether it originates from a PersistentVolume annotation or the Pod's specification, is applied to the first process run in each container."
   ]
  },
  {
   "cell_type": "raw",
   "id": "58035abf",
   "metadata": {},
   "source": [
    "Note: When a Pod consumes a PersistentVolume, the GIDs associated with the PersistentVolume are not present on the Pod resource itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
